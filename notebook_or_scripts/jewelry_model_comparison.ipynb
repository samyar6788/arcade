{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udc8e Jewelry Generation: SDXL vs SD1.5 Model Comparison\n",
    "\n",
    "## Comprehensive Experiment Design\n",
    "\n",
    "This notebook tests **three enhancement methods** across **two models** for jewelry image generation:\n",
    "\n",
    "### \ud83d\udd2c **Enhancement Methods:**\n",
    "1. **Baseline** - No enhancements\n",
    "2. **Compel Weighting** - Using `++` syntax for term emphasis  \n",
    "3. **Word Replacement** - Enhanced jewelry terminology (channel-set \u2192 \"channel-set groove set gems\")\n",
    "\n",
    "### \ud83e\udd16 **Models Tested:**\n",
    "- **SDXL** (`stabilityai/stable-diffusion-xl-base-1.0`)\n",
    "- **SD1.5** (`runwayml/stable-diffusion-v1-5`)\n",
    "\n",
    "### \ud83d\udcca **Evaluation:**\n",
    "- **Visual comparison** - Side-by-side image analysis\n",
    "- **CLIP similarity** - Quantitative prompt adherence scoring\n",
    "- **CSV export** - All prompt variations for analysis\n",
    "\n",
    "### \ud83c\udfaf **Research Questions:**\n",
    "1. Which enhancement method works best?\n",
    "2. Which model responds better to enhancements?\n",
    "3. Can we improve specific issues like engraved letter visibility?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install compel\n",
    "!pip install open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch pandas\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\ud83d\udda5\ufe0f  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\ud83d\udcbe Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"model_comparison_results\", exist_ok=True)\n",
    "os.makedirs(\"model_comparison_results/sdxl\", exist_ok=True)\n",
    "os.makedirs(\"model_comparison_results/sd15\", exist_ok=True)\n",
    "print(\"\u2705 Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Test Prompts & Enhancement Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 8 test prompts from the assignment\n",
    "test_prompts = [\n",
    "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
    "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
    "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
    "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
    "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
    "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
    "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav\u00e9, editorial lighting\",\n",
    "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
    "]\n",
    "\n",
    "def create_compel_enhanced_prompt(prompt):\n",
    "    \"\"\"Add ++ weighting to critical jewelry terms for Compel\"\"\"\n",
    "    critical_terms = {\n",
    "        \"channel-set\": \"channel-set++\",\n",
    "        \"threader\": \"threader++\", \n",
    "        \"bezel-set\": \"bezel-set++\",\n",
    "        \"eternity band\": \"eternity band++\",\n",
    "        \"huggie\": \"huggie++\",\n",
    "        \"bypass\": \"bypass++\",\n",
    "        \"pav\u00e9\": \"pav\u00e9++\",\n",
    "        \"signet\": \"signet++\",\n",
    "        \"cuff\": \"cuff++\",\n",
    "        \"cluster\": \"cluster++\",\n",
    "        \"diamond\": \"diamond++\",\n",
    "        \"sapphire\": \"sapphire++\",\n",
    "        \"gold\": \"gold++\",\n",
    "        \"platinum\": \"platinum++\",\n",
    "        \"engraved\": \"engraved++\",\n",
    "        \"initial\": \"initial++\",\n",
    "        \"'M'\": \"'M'++\"\n",
    "    }\n",
    "    \n",
    "    enhanced = prompt\n",
    "    for term, weighted in critical_terms.items():\n",
    "        if term in prompt.lower():\n",
    "            enhanced = enhanced.replace(term, weighted)\n",
    "    return enhanced\n",
    "\n",
    "def create_word_replacement_enhanced_prompt(prompt):\n",
    "    \"\"\"Enhanced jewelry terminology (from your existing pipeline)\"\"\"\n",
    "    jewelry_terms = {\n",
    "        \"channel-set\": \"channel-set (parallel groove gemstones)\",\n",
    "        \"threader\": \"threader (thread-through earring)\",\n",
    "        \"bezel-set\": \"bezel-set (rim-enclosed gemstone)\",\n",
    "        \"eternity band\": \"eternity band (full-band gemstones)\",\n",
    "        \"huggie\": \"huggie (small close hoop)\",\n",
    "        \"bypass\": \"bypass (overlapping band ring)\",\n",
    "        \"pav\u00e9\": \"pav\u00e9 (small set stones)\",\n",
    "        \"signet\": \"signet (flat engraved ring)\",\n",
    "        \"cuff\": \"cuff (open bracelet)\",\n",
    "        \"cluster\": \"cluster (grouped gemstones)\"\n",
    "    }\n",
    "    \n",
    "    enhanced = prompt\n",
    "    for term, description in jewelry_terms.items():\n",
    "        if term in prompt.lower():\n",
    "            enhanced = enhanced.replace(term, description)\n",
    "    \n",
    "    # Add modern aesthetic terms\n",
    "    enhanced += \", high-end jewelry, luxury craftsmanship, premium materials\"\n",
    "    return enhanced\n",
    "\n",
    "# Create all prompt variations\n",
    "compel_prompts = [create_compel_enhanced_prompt(p) for p in test_prompts]\n",
    "word_replacement_prompts = [create_word_replacement_enhanced_prompt(p) for p in test_prompts]\n",
    "\n",
    "# Common negative prompt\n",
    "negative_prompt = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly\"\n",
    "\n",
    "print(f\"\u2705 {len(test_prompts)} test prompts prepared with 3 enhancement variations each\")\n",
    "print(f\"\ud83d\udcca Total prompt combinations: {len(test_prompts)} \u00d7 3 methods \u00d7 2 models = {len(test_prompts) * 3 * 2} generations\")\n",
    "\n",
    "# Display first prompt as example\n",
    "print(f\"\\n\ud83d\udcdd Example (Prompt 1):\")\n",
    "print(f\"Original:        {test_prompts[0]}\")\n",
    "print(f\"Compel:          {compel_prompts[0]}\")\n",
    "print(f\"Word Enhanced:   {word_replacement_prompts[0][:100]}...\")\n",
    "print(f\"Negative:        {negative_prompt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd16 Model Loading & Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load either SDXL or SD1.5 with corresponding Compel instance\"\"\"\n",
    "    if model_name == \"SDXL\":\n",
    "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            model_id, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16\n",
    "        ).to(device)\n",
    "        compel_inst = Compel(\n",
    "            tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
    "            text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
    "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "            requires_pooled=[False, True],\n",
    "        )\n",
    "        return pipe, compel_inst, True  # True = is_sdxl\n",
    "    else:  # SD1.5\n",
    "        model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
    "        ).to(device)\n",
    "        compel_inst = Compel(\n",
    "            tokenizer=pipe.tokenizer,\n",
    "            text_encoder=pipe.text_encoder,\n",
    "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "            requires_pooled=False,\n",
    "        )\n",
    "        return pipe, compel_inst, False  # False = not_sdxl\n",
    "\n",
    "def generate_image(pipe, compel_inst, is_sdxl, prompt, method=\"baseline\", seed=42):\n",
    "    \"\"\"Generate image with specified method: baseline, compel, or word_replacement\"\"\"\n",
    "    \n",
    "    # Set optimal parameters for each model\n",
    "    steps = 30\n",
    "    cfg = 5.0 if is_sdxl else 7.5\n",
    "    w, h = (1024, 1024) if is_sdxl else (768, 768)\n",
    "    \n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    if method == \"baseline\":\n",
    "        # Standard generation\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=cfg,\n",
    "            width=w, height=h,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "        \n",
    "    elif method == \"compel\":\n",
    "        # Compel-enhanced generation\n",
    "        if is_sdxl:\n",
    "            # SDXL: dual encoders\n",
    "            cond, pooled = compel_inst([prompt, negative_prompt])\n",
    "            image = pipe(\n",
    "                prompt_embeds=cond[0:1], \n",
    "                pooled_prompt_embeds=pooled[0:1],\n",
    "                negative_prompt_embeds=cond[1:2], \n",
    "                negative_pooled_prompt_embeds=pooled[1:2],\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=cfg,\n",
    "                width=w, height=h,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        else:\n",
    "            # SD1.5: single encoder\n",
    "            pos_cond = compel_inst.build_conditioning_tensor(prompt)\n",
    "            neg_cond = compel_inst.build_conditioning_tensor(negative_prompt)\n",
    "            image = pipe(\n",
    "                prompt_embeds=pos_cond,\n",
    "                negative_prompt_embeds=neg_cond,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=cfg,\n",
    "                width=w, height=h,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "            \n",
    "    elif method == \"word_replacement\":\n",
    "        # Standard generation with enhanced prompt\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=cfg,\n",
    "            width=w, height=h,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"\u2705 Model loading and generation functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Run Full Experiment\n",
    "\n",
    "This will generate images for all combinations:\n",
    "- **2 models** \u00d7 **8 prompts** \u00d7 **3 methods** = **48 total images**\n",
    "- Estimated time: 20-40 minutes depending on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive experiment\n",
    "models_to_test = [\"SDXL\", \"SD15\"]\n",
    "methods = [\"baseline\", \"compel\", \"word_replacement\"]\n",
    "\n",
    "# Store all results for analysis\n",
    "all_results = []\n",
    "\n",
    "print(\"\ud83d\ude80 Starting comprehensive jewelry generation experiment...\")\n",
    "print(f\"\u23f1\ufe0f  Estimated time: {len(models_to_test) * len(test_prompts) * len(methods) * 2} minutes\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n\ud83e\udd16 Loading {model_name}...\")\n",
    "    pipe, compel_inst, is_sdxl = load_model(model_name)\n",
    "    \n",
    "    for prompt_idx, base_prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n\ud83d\udcdd Prompt {prompt_idx}/8: {base_prompt[:50]}...\")\n",
    "        \n",
    "        # Get prompt variations\n",
    "        prompts = {\n",
    "            \"baseline\": base_prompt,\n",
    "            \"compel\": compel_prompts[prompt_idx-1],\n",
    "            \"word_replacement\": word_replacement_prompts[prompt_idx-1]\n",
    "        }\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                print(f\"  \ud83c\udfa8 Generating {method}...\")\n",
    "                \n",
    "                # Generate image\n",
    "                image = generate_image(\n",
    "                    pipe, compel_inst, is_sdxl, \n",
    "                    prompts[method], method, \n",
    "                    seed=100 + prompt_idx\n",
    "                )\n",
    "                \n",
    "                # Save image\n",
    "                filename = f\"{model_name.lower()}/p{prompt_idx:02d}_{method}.png\"\n",
    "                filepath = f\"model_comparison_results/{filename}\"\n",
    "                image.save(filepath)\n",
    "                \n",
    "                # Store result for analysis\n",
    "                result = {\n",
    "                    'model': model_name,\n",
    "                    'prompt_id': prompt_idx,\n",
    "                    'method': method,\n",
    "                    'original_prompt': base_prompt,\n",
    "                    'used_prompt': prompts[method],\n",
    "                    'image_path': filepath,\n",
    "                    'image': image\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "                print(f\"    \u2705 Saved: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    \u274c Error in {method}: {e}\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del pipe, compel_inst\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\u2705 {model_name} completed!\")\n",
    "\n",
    "print(f\"\\n\ud83c\udf89 Experiment completed! Generated {len(all_results)} images\")\n",
    "print(f\"\ud83d\udcc1 Results saved in: model_comparison_results/\")\n",
    "print(f\"\ud83d\udcca Ready for analysis and comparison!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Export Results to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive CSV with all prompt variations\n",
    "def export_results_to_csv():\n",
    "    \"\"\"Export all prompt variations and results to CSV for analysis\"\"\"\n",
    "    \n",
    "    # Create data for CSV\n",
    "    csv_data = []\n",
    "    \n",
    "    for prompt_idx, base_prompt in enumerate(test_prompts, 1):\n",
    "        compel_prompt = compel_prompts[prompt_idx-1]\n",
    "        word_prompt = word_replacement_prompts[prompt_idx-1]\n",
    "        \n",
    "        # Find generated results for this prompt\n",
    "        prompt_results = [r for r in all_results if r['prompt_id'] == prompt_idx]\n",
    "        \n",
    "        # Group by model and method\n",
    "        sdxl_results = {r['method']: r['image_path'] for r in prompt_results if r['model'] == 'SDXL'}\n",
    "        sd15_results = {r['method']: r['image_path'] for r in prompt_results if r['model'] == 'SD15'}\n",
    "        \n",
    "        row = {\n",
    "            'prompt_id': prompt_idx,\n",
    "            'original_prompt': base_prompt,\n",
    "            'compel_enhanced_prompt': compel_prompt,\n",
    "            'word_replacement_enhanced_prompt': word_prompt,\n",
    "            'negative_prompt': negative_prompt,\n",
    "            \n",
    "            # SDXL file paths\n",
    "            'sdxl_baseline_path': sdxl_results.get('baseline', ''),\n",
    "            'sdxl_compel_path': sdxl_results.get('compel', ''),\n",
    "            'sdxl_word_replacement_path': sdxl_results.get('word_replacement', ''),\n",
    "            \n",
    "            # SD15 file paths  \n",
    "            'sd15_baseline_path': sd15_results.get('baseline', ''),\n",
    "            'sd15_compel_path': sd15_results.get('compel', ''),\n",
    "            'sd15_word_replacement_path': sd15_results.get('word_replacement', ''),\n",
    "            \n",
    "            # Enhancement analysis\n",
    "            'compel_changes': ', '.join([f'{k}++' for k in ['channel-set', 'diamond', 'gold', 'platinum', 'engraved', 'signet'] if k in base_prompt.lower()]),\n",
    "            'word_replacements': ', '.join([k for k in ['channel-set', 'threader', 'bezel-set', 'huggie', 'cuff'] if k in base_prompt.lower()])\n",
    "        }\n",
    "        csv_data.append(row)\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = \"model_comparison_results/comprehensive_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\ud83d\udcca CSV Export Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\ud83d\udcbe Saved to: {csv_path}\")\n",
    "    print(f\"\ud83d\udccb Total prompts: {len(df)}\")\n",
    "    print(f\"\ud83c\udfdb\ufe0f Columns: {len(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcdd Column breakdown:\")\n",
    "    prompt_cols = [c for c in df.columns if 'prompt' in c]\n",
    "    path_cols = [c for c in df.columns if 'path' in c]\n",
    "    analysis_cols = [c for c in df.columns if c in ['compel_changes', 'word_replacements']]\n",
    "    \n",
    "    print(f\"  Prompts: {len(prompt_cols)} ({', '.join(prompt_cols)})\")\n",
    "    print(f\"  Paths: {len(path_cols)} (sdxl/sd15 \u00d7 baseline/compel/word_replacement)\")\n",
    "    print(f\"  Analysis: {len(analysis_cols)} (enhancement tracking)\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\n\ud83d\udccb Sample data (first 2 rows):\")\n",
    "    display_cols = ['prompt_id', 'original_prompt', 'compel_enhanced_prompt', 'sdxl_baseline_path']\n",
    "    print(df[display_cols].head(2).to_string(max_colwidth=50))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Export to CSV\n",
    "if 'all_results' in locals() and all_results:\n",
    "    results_df = export_results_to_csv()\n",
    "    print(f\"\\n\u2705 CSV export completed with {len(results_df)} prompt variations!\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No results to export - run the experiment first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Comprehensive Evaluation: CLIP + LAION Aesthetic Scoring\n",
    "\n",
    "This section provides quantitative evaluation using two complementary metrics:\n",
    "- **CLIP Similarity** - Measures prompt adherence (how well the image matches the text)\n",
    "- **LAION Aesthetic Score** - Measures visual quality and aesthetic appeal\n",
    "\n",
    "The combination gives us both technical accuracy and aesthetic quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation: CLIP similarity + LAION aesthetic scoring\n",
    "try:\n",
    "    import open_clip\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    print(\"\ud83d\udcca Loading evaluation models...\")\n",
    "    \n",
    "    # CLIP model for prompt adherence\n",
    "    print(\"  \ud83d\udd0d Loading CLIP model...\")\n",
    "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "    clip_model = clip_model.to(device).eval()\n",
    "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    \n",
    "    # LAION aesthetic predictor for image quality/aesthetics\n",
    "    print(\"  \ud83c\udfa8 Loading LAION aesthetic model...\")\n",
    "    try:\n",
    "        aesthetic_model = pipeline(\n",
    "            \"image-classification\", \n",
    "            model=\"cafeai/cafe_aesthetic\", \n",
    "            device=0 if device == \"cuda\" else -1\n",
    "        )\n",
    "        aesthetic_available = True\n",
    "        print(\"    \u2705 LAION aesthetic model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"    \u26a0\ufe0f LAION aesthetic model failed to load: {e}\")\n",
    "        print(\"    \ud83d\udcdd Continuing with CLIP evaluation only\")\n",
    "        aesthetic_available = False\n",
    "    \n",
    "    def calculate_clip_similarity(image, text):\n",
    "        \"\"\"Calculate CLIP similarity between image and text (prompt adherence)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "            text_input = clip_tokenizer([text]).to(device)\n",
    "            \n",
    "            image_features = clip_model.encode_image(image_input)\n",
    "            text_features = clip_model.encode_text(text_input)\n",
    "            \n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            similarity = (image_features @ text_features.T).squeeze().item()\n",
    "            return float(similarity)\n",
    "    \n",
    "    def calculate_aesthetic_score(image):\n",
    "        \"\"\"Calculate LAION aesthetic score (image quality/beauty)\"\"\"\n",
    "        if not aesthetic_available:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Get aesthetic predictions\n",
    "            predictions = aesthetic_model(image)\n",
    "            \n",
    "            # Extract aesthetic score (higher = better aesthetics)\n",
    "            # The cafe_aesthetic model typically returns aesthetic/not_aesthetic scores\n",
    "            for pred in predictions:\n",
    "                if 'aesthetic' in pred['label'].lower() and 'not' not in pred['label'].lower():\n",
    "                    return float(pred['score'])\n",
    "            \n",
    "            # Fallback: return highest score if structure is different\n",
    "            return float(max(predictions, key=lambda x: x['score'])['score'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    \u26a0\ufe0f Aesthetic scoring failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Evaluate all results\n",
    "    if 'all_results' in locals() and all_results:\n",
    "        print(\"\\n\ud83d\udcca Comprehensive Evaluation Results:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group results by model and method\n",
    "        evaluation_summary = {}\n",
    "        aesthetic_summary = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            model = result['model']\n",
    "            method = result['method']\n",
    "            key = f\"{model}_{method}\"\n",
    "            \n",
    "            if key not in evaluation_summary:\n",
    "                evaluation_summary[key] = []\n",
    "                aesthetic_summary[key] = []\n",
    "            \n",
    "            # Calculate CLIP similarity with original prompt\n",
    "            similarity = calculate_clip_similarity(result['image'], result['original_prompt'])\n",
    "            evaluation_summary[key].append(similarity)\n",
    "            \n",
    "            # Calculate aesthetic score\n",
    "            aesthetic_score = calculate_aesthetic_score(result['image'])\n",
    "            if aesthetic_score is not None:\n",
    "                aesthetic_summary[key].append(aesthetic_score)\n",
    "        \n",
    "        # Display CLIP results\n",
    "        print(f\"\\n\ud83d\udd0d CLIP Similarity (Prompt Adherence):\")\n",
    "        print(f\"{'Model':<8} {'Method':<15} {'Avg CLIP':<10} {'Std':<8} {'Samples':<8}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for key, scores in evaluation_summary.items():\n",
    "            model, method = key.split('_', 1)\n",
    "            avg_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            \n",
    "            print(f\"{model:<8} {method:<15} {avg_score:.3f}     {std_score:.3f}   {len(scores)}\")\n",
    "        \n",
    "        # Display aesthetic results if available\n",
    "        if aesthetic_available and any(aesthetic_summary.values()):\n",
    "            print(f\"\\n\ud83c\udfa8 LAION Aesthetic Scores (Visual Quality):\")\n",
    "            print(f\"{'Model':<8} {'Method':<15} {'Avg Aesthetic':<13} {'Std':<8} {'Samples':<8}\")\n",
    "            print(\"-\" * 65)\n",
    "            \n",
    "            for key, scores in aesthetic_summary.items():\n",
    "                if scores:  # Only show if we have scores\n",
    "                    model, method = key.split('_', 1)\n",
    "                    avg_score = np.mean(scores)\n",
    "                    std_score = np.std(scores)\n",
    "                    \n",
    "                    print(f\"{model:<8} {method:<15} {avg_score:.3f}        {std_score:.3f}   {len(scores)}\")\n",
    "        \n",
    "        # Compare methods within each model\n",
    "        print(f\"\\n\ud83d\udcc8 Model Comparison:\")\n",
    "        for model in ['SDXL', 'SD15']:\n",
    "            print(f\"\\n{model}:\")\n",
    "            baseline_scores = evaluation_summary.get(f'{model}_baseline', [])\n",
    "            compel_scores = evaluation_summary.get(f'{model}_compel', [])\n",
    "            word_scores = evaluation_summary.get(f'{model}_word_replacement', [])\n",
    "            \n",
    "            # CLIP improvements\n",
    "            if baseline_scores and compel_scores:\n",
    "                compel_improvement = np.mean(compel_scores) - np.mean(baseline_scores)\n",
    "                print(f\"  \ud83d\udcca CLIP - Compel vs Baseline: {compel_improvement:+.3f}\")\n",
    "            \n",
    "            if baseline_scores and word_scores:\n",
    "                word_improvement = np.mean(word_scores) - np.mean(baseline_scores)\n",
    "                print(f\"  \ud83d\udcca CLIP - Word Enhancement vs Baseline: {word_improvement:+.3f}\")\n",
    "            \n",
    "            # Aesthetic improvements if available\n",
    "            if aesthetic_available:\n",
    "                baseline_aes = aesthetic_summary.get(f'{model}_baseline', [])\n",
    "                compel_aes = aesthetic_summary.get(f'{model}_compel', [])\n",
    "                word_aes = aesthetic_summary.get(f'{model}_word_replacement', [])\n",
    "                \n",
    "                if baseline_aes and compel_aes:\n",
    "                    aes_improvement = np.mean(compel_aes) - np.mean(baseline_aes)\n",
    "                    print(f\"  \ud83c\udfa8 Aesthetic - Compel vs Baseline: {aes_improvement:+.3f}\")\n",
    "                \n",
    "                if baseline_aes and word_aes:\n",
    "                    aes_improvement = np.mean(word_aes) - np.mean(baseline_aes)\n",
    "                    print(f\"  \ud83c\udfa8 Aesthetic - Word Enhancement vs Baseline: {aes_improvement:+.3f}\")\n",
    "        \n",
    "        print(\"\\n\u2705 Comprehensive evaluation completed!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No results to evaluate - run the experiment first!\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f  Required packages not installed - skipping evaluation\")\n",
    "    print(\"   Install with: pip install open-clip-torch transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error in evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Summary & Next Steps\n",
    "\n",
    "### \ud83d\udcc1 **Generated Files:**\n",
    "- `model_comparison_results/sdxl/p01_baseline.png` - SDXL baseline images\n",
    "- `model_comparison_results/sdxl/p01_compel.png` - SDXL Compel-enhanced images  \n",
    "- `model_comparison_results/sdxl/p01_word_replacement.png` - SDXL word-enhanced images\n",
    "- `model_comparison_results/sd15/p01_baseline.png` - SD1.5 baseline images\n",
    "- `model_comparison_results/sd15/p01_compel.png` - SD1.5 Compel-enhanced images\n",
    "- `model_comparison_results/sd15/p01_word_replacement.png` - SD1.5 word-enhanced images  \n",
    "- `model_comparison_results/comprehensive_results.csv` - **Complete analysis CSV**\n",
    "\n",
    "### \ud83d\udd0d **Analysis Questions:**\n",
    "1. **Which model performs better overall?** (SDXL vs SD1.5)\n",
    "2. **Which enhancement method is most effective?** (Compel vs Word Replacement)\n",
    "3. **Are there specific jewelry terms that benefit more from certain enhancements?**\n",
    "4. **Does the engraved 'M' visibility improve with any method?**\n",
    "\n",
    "### \ud83c\udfaf **Key Findings to Look For:**\n",
    "- **Visual quality differences** between models and methods\n",
    "- **LAION aesthetic scores** measuring visual quality and appeal\n",
    "- **CLIP similarity improvements** indicating better prompt adherence\n",
    "- **Specific jewelry features** rendered more accurately\n",
    "- **Style consistency** (modern vs vintage aesthetic)\n",
    "\n",
    "### \ud83d\udca1 **Next Steps:**\n",
    "1. **Visual inspection** - Compare generated images side by side\n",
    "2. **CSV analysis** - Use spreadsheet tools for systematic comparison  \n",
    "3. **CLIP scores** - Review quantitative improvements\n",
    "4. **Method selection** - Choose best-performing combination for production\n",
    "5. **Integration** - Implement winning approach in main pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udfc6 Goal**: Determine the optimal model + enhancement combination for high-quality jewelry image generation!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}