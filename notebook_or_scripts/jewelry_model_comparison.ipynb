{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üíé Jewelry Generation: SDXL vs SD1.5 Model Comparison\n",
        "\n",
        "## Comprehensive Experiment Design\n",
        "\n",
        "This notebook tests **three enhancement methods** across **two models** for jewelry image generation:\n",
        "\n",
        "### üî¨ **Enhancement Methods:**\n",
        "1. **Baseline** - No enhancements\n",
        "2. **Compel Weighting** - Using `++` syntax for term emphasis  \n",
        "3. **Word Replacement** - Enhanced jewelry terminology (channel-set ‚Üí \"channel-set groove set gems\")\n",
        "\n",
        "### ü§ñ **Models Tested:**\n",
        "- **SDXL** (`stabilityai/stable-diffusion-xl-base-1.0`)\n",
        "- **SD1.5** (`runwayml/stable-diffusion-v1-5`)\n",
        "\n",
        "### üìä **Evaluation:**\n",
        "- **Visual comparison** - Side-by-side image analysis\n",
        "- **CLIP similarity** - Quantitative prompt adherence scoring\n",
        "- **CSV export** - All prompt variations for analysis\n",
        "\n",
        "### üéØ **Research Questions:**\n",
        "1. Which enhancement method works best?\n",
        "2. Which model responds better to enhancements?\n",
        "3. Can we improve specific issues like engraved letter visibility?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Dependencies\n",
        "!pip install compel\n",
        "!pip install open-clip-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (uncomment for Colab)\n",
        "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch pandas\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(\"model_comparison_results\", exist_ok=True)\n",
        "os.makedirs(\"model_comparison_results/sdxl\", exist_ok=True)\n",
        "os.makedirs(\"model_comparison_results/sd15\", exist_ok=True)\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Test Prompts & Enhancement Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 8 test prompts from the assignment\n",
        "test_prompts = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav√©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "def create_compel_enhanced_prompt(prompt):\n",
        "    \"\"\"Add ++ weighting to critical jewelry terms for Compel\"\"\"\n",
        "    critical_terms = {\n",
        "        \"channel-set\": \"channel-set++\",\n",
        "        \"threader\": \"threader++\", \n",
        "        \"bezel-set\": \"bezel-set++\",\n",
        "        \"eternity band\": \"eternity band++\",\n",
        "        \"huggie\": \"huggie++\",\n",
        "        \"bypass\": \"bypass++\",\n",
        "        \"pav√©\": \"pav√©++\",\n",
        "        \"signet\": \"signet++\",\n",
        "        \"cuff\": \"cuff++\",\n",
        "        \"cluster\": \"cluster++\",\n",
        "        \"diamond\": \"diamond++\",\n",
        "        \"sapphire\": \"sapphire++\",\n",
        "        \"gold\": \"gold++\",\n",
        "        \"platinum\": \"platinum++\",\n",
        "        \"engraved\": \"engraved++\",\n",
        "        \"initial\": \"initial++\",\n",
        "        \"'M'\": \"'M'++\"\n",
        "    }\n",
        "    \n",
        "    enhanced = prompt\n",
        "    for term, weighted in critical_terms.items():\n",
        "        if term in prompt.lower():\n",
        "            enhanced = enhanced.replace(term, weighted)\n",
        "    return enhanced\n",
        "\n",
        "def create_word_replacement_enhanced_prompt(prompt):\n",
        "    \"\"\"Enhanced jewelry terminology (from your existing pipeline)\"\"\"\n",
        "    jewelry_terms = {\n",
        "        \"channel-set\": \"channel-set (parallel groove gemstones)\",\n",
        "        \"threader\": \"threader (thread-through earring)\",\n",
        "        \"bezel-set\": \"bezel-set (rim-enclosed gemstone)\",\n",
        "        \"eternity band\": \"eternity band (full-band gemstones)\",\n",
        "        \"huggie\": \"huggie (small close hoop)\",\n",
        "        \"bypass\": \"bypass (overlapping band ring)\",\n",
        "        \"pav√©\": \"pav√© (small set stones)\",\n",
        "        \"signet\": \"signet (flat engraved ring)\",\n",
        "        \"cuff\": \"cuff (open bracelet)\",\n",
        "        \"cluster\": \"cluster (grouped gemstones)\"\n",
        "    }\n",
        "    \n",
        "    enhanced = prompt\n",
        "    for term, description in jewelry_terms.items():\n",
        "        if term in prompt.lower():\n",
        "            enhanced = enhanced.replace(term, description)\n",
        "    \n",
        "    # Add modern aesthetic terms\n",
        "    enhanced += \", high-end jewelry, luxury craftsmanship, premium materials\"\n",
        "    return enhanced\n",
        "\n",
        "# Create all prompt variations\n",
        "compel_prompts = [create_compel_enhanced_prompt(p) for p in test_prompts]\n",
        "word_replacement_prompts = [create_word_replacement_enhanced_prompt(p) for p in test_prompts]\n",
        "\n",
        "# Common negative prompt\n",
        "negative_prompt = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly\"\n",
        "\n",
        "print(f\"‚úÖ {len(test_prompts)} test prompts prepared with 3 enhancement variations each\")\n",
        "print(f\"üìä Total prompt combinations: {len(test_prompts)} √ó 3 methods √ó 2 models = {len(test_prompts) * 3 * 2} generations\")\n",
        "\n",
        "# Display first prompt as example\n",
        "print(f\"\\nüìù Example (Prompt 1):\")\n",
        "print(f\"Original:        {test_prompts[0]}\")\n",
        "print(f\"Compel:          {compel_prompts[0]}\")\n",
        "print(f\"Word Enhanced:   {word_replacement_prompts[0][:100]}...\")\n",
        "print(f\"Negative:        {negative_prompt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Model Loading & Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"Load either SDXL or SD1.5 with corresponding Compel instance\"\"\"\n",
        "    if model_name == \"SDXL\":\n",
        "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_id, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
        "            text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=[False, True],\n",
        "        )\n",
        "        return pipe, compel_inst, True  # True = is_sdxl\n",
        "    else:  # SD1.5\n",
        "        model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
        "        ).to(device)\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=pipe.tokenizer,\n",
        "            text_encoder=pipe.text_encoder,\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=False,\n",
        "        )\n",
        "        return pipe, compel_inst, False  # False = not_sdxl\n",
        "\n",
        "def generate_image(pipe, compel_inst, is_sdxl, prompt, method=\"baseline\", seed=42):\n",
        "    \"\"\"Generate image with specified method: baseline, compel, or word_replacement\"\"\"\n",
        "    \n",
        "    # Set optimal parameters for each model\n",
        "    steps = 30\n",
        "    cfg = 5.0 if is_sdxl else 7.5\n",
        "    w, h = (1024, 1024) if is_sdxl else (768, 768)\n",
        "    \n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    if method == \"baseline\":\n",
        "        # Standard generation\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            width=w, height=h,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "        \n",
        "    elif method == \"compel\":\n",
        "        # Compel-enhanced generation\n",
        "        if is_sdxl:\n",
        "            # SDXL: dual encoders\n",
        "            cond, pooled = compel_inst([prompt, negative_prompt])\n",
        "            image = pipe(\n",
        "                prompt_embeds=cond[0:1], \n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=cond[1:2], \n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=cfg,\n",
        "                width=w, height=h,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "        else:\n",
        "            # SD1.5: single encoder\n",
        "            pos_cond = compel_inst.build_conditioning_tensor(prompt)\n",
        "            neg_cond = compel_inst.build_conditioning_tensor(negative_prompt)\n",
        "            image = pipe(\n",
        "                prompt_embeds=pos_cond,\n",
        "                negative_prompt_embeds=neg_cond,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=cfg,\n",
        "                width=w, height=h,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "            \n",
        "    elif method == \"word_replacement\":\n",
        "        # Standard generation with enhanced prompt\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            width=w, height=h,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "print(\"‚úÖ Model loading and generation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Run Full Experiment\n",
        "\n",
        "This will generate images for all combinations:\n",
        "- **2 models** √ó **8 prompts** √ó **3 methods** = **48 total images**\n",
        "- Estimated time: 20-40 minutes depending on GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive experiment\n",
        "models_to_test = [\"SDXL\", \"SD15\"]\n",
        "methods = [\"baseline\", \"compel\", \"word_replacement\"]\n",
        "\n",
        "# Store all results for analysis\n",
        "all_results = []\n",
        "\n",
        "print(\"üöÄ Starting comprehensive jewelry generation experiment...\")\n",
        "print(f\"‚è±Ô∏è  Estimated time: {len(models_to_test) * len(test_prompts) * len(methods) * 2} minutes\")\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\nü§ñ Loading {model_name}...\")\n",
        "    pipe, compel_inst, is_sdxl = load_model(model_name)\n",
        "    \n",
        "    for prompt_idx, base_prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\nüìù Prompt {prompt_idx}/8: {base_prompt[:50]}...\")\n",
        "        \n",
        "        # Get prompt variations\n",
        "        prompts = {\n",
        "            \"baseline\": base_prompt,\n",
        "            \"compel\": compel_prompts[prompt_idx-1],\n",
        "            \"word_replacement\": word_replacement_prompts[prompt_idx-1]\n",
        "        }\n",
        "        \n",
        "        for method in methods:\n",
        "            try:\n",
        "                print(f\"  üé® Generating {method}...\")\n",
        "                \n",
        "                # Generate image\n",
        "                image = generate_image(\n",
        "                    pipe, compel_inst, is_sdxl, \n",
        "                    prompts[method], method, \n",
        "                    seed=100 + prompt_idx\n",
        "                )\n",
        "                \n",
        "                # Save image\n",
        "                filename = f\"{model_name.lower()}/p{prompt_idx:02d}_{method}.png\"\n",
        "                filepath = f\"model_comparison_results/{filename}\"\n",
        "                image.save(filepath)\n",
        "                \n",
        "                # Store result for analysis\n",
        "                result = {\n",
        "                    'model': model_name,\n",
        "                    'prompt_id': prompt_idx,\n",
        "                    'method': method,\n",
        "                    'original_prompt': base_prompt,\n",
        "                    'used_prompt': prompts[method],\n",
        "                    'image_path': filepath,\n",
        "                    'image': image\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                \n",
        "                print(f\"    ‚úÖ Saved: {filename}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå Error in {method}: {e}\")\n",
        "    \n",
        "    # Clear GPU memory\n",
        "    del pipe, compel_inst\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"‚úÖ {model_name} completed!\")\n",
        "\n",
        "print(f\"\\nüéâ Experiment completed! Generated {len(all_results)} images\")\n",
        "print(f\"üìÅ Results saved in: model_comparison_results/\")\n",
        "print(f\"üìä Ready for analysis and comparison!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Export Results to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive CSV with all prompt variations\n",
        "def export_results_to_csv():\n",
        "    \"\"\"Export all prompt variations and results to CSV for analysis\"\"\"\n",
        "    \n",
        "    # Create data for CSV\n",
        "    csv_data = []\n",
        "    \n",
        "    for prompt_idx, base_prompt in enumerate(test_prompts, 1):\n",
        "        compel_prompt = compel_prompts[prompt_idx-1]\n",
        "        word_prompt = word_replacement_prompts[prompt_idx-1]\n",
        "        \n",
        "        # Find generated results for this prompt\n",
        "        prompt_results = [r for r in all_results if r['prompt_id'] == prompt_idx]\n",
        "        \n",
        "        # Group by model and method\n",
        "        sdxl_results = {r['method']: r['image_path'] for r in prompt_results if r['model'] == 'SDXL'}\n",
        "        sd15_results = {r['method']: r['image_path'] for r in prompt_results if r['model'] == 'SD15'}\n",
        "        \n",
        "        row = {\n",
        "            'prompt_id': prompt_idx,\n",
        "            'original_prompt': base_prompt,\n",
        "            'compel_enhanced_prompt': compel_prompt,\n",
        "            'word_replacement_enhanced_prompt': word_prompt,\n",
        "            'negative_prompt': negative_prompt,\n",
        "            \n",
        "            # SDXL file paths\n",
        "            'sdxl_baseline_path': sdxl_results.get('baseline', ''),\n",
        "            'sdxl_compel_path': sdxl_results.get('compel', ''),\n",
        "            'sdxl_word_replacement_path': sdxl_results.get('word_replacement', ''),\n",
        "            \n",
        "            # SD15 file paths  \n",
        "            'sd15_baseline_path': sd15_results.get('baseline', ''),\n",
        "            'sd15_compel_path': sd15_results.get('compel', ''),\n",
        "            'sd15_word_replacement_path': sd15_results.get('word_replacement', ''),\n",
        "            \n",
        "            # Enhancement analysis\n",
        "            'compel_changes': ', '.join([f'{k}++' for k in ['channel-set', 'diamond', 'gold', 'platinum', 'engraved', 'signet'] if k in base_prompt.lower()]),\n",
        "            'word_replacements': ', '.join([k for k in ['channel-set', 'threader', 'bezel-set', 'huggie', 'cuff'] if k in base_prompt.lower()])\n",
        "        }\n",
        "        csv_data.append(row)\n",
        "    \n",
        "    # Create DataFrame and save\n",
        "    df = pd.DataFrame(csv_data)\n",
        "    csv_path = \"model_comparison_results/comprehensive_results.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"üìä CSV Export Summary:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üíæ Saved to: {csv_path}\")\n",
        "    print(f\"üìã Total prompts: {len(df)}\")\n",
        "    print(f\"üèõÔ∏è Columns: {len(df.columns)}\")\n",
        "    \n",
        "    print(f\"\\nüìù Column breakdown:\")\n",
        "    prompt_cols = [c for c in df.columns if 'prompt' in c]\n",
        "    path_cols = [c for c in df.columns if 'path' in c]\n",
        "    analysis_cols = [c for c in df.columns if c in ['compel_changes', 'word_replacements']]\n",
        "    \n",
        "    print(f\"  Prompts: {len(prompt_cols)} ({', '.join(prompt_cols)})\")\n",
        "    print(f\"  Paths: {len(path_cols)} (sdxl/sd15 √ó baseline/compel/word_replacement)\")\n",
        "    print(f\"  Analysis: {len(analysis_cols)} (enhancement tracking)\")\n",
        "    \n",
        "    # Show sample data\n",
        "    print(f\"\\nüìã Sample data (first 2 rows):\")\n",
        "    display_cols = ['prompt_id', 'original_prompt', 'compel_enhanced_prompt', 'sdxl_baseline_path']\n",
        "    print(df[display_cols].head(2).to_string(max_colwidth=50))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Export to CSV\n",
        "if 'all_results' in locals() and all_results:\n",
        "    results_df = export_results_to_csv()\n",
        "    print(f\"\\n‚úÖ CSV export completed with {len(results_df)} prompt variations!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to export - run the experiment first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Optional: CLIP Similarity Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional CLIP evaluation to measure prompt adherence quantitatively\n",
        "try:\n",
        "    import open_clip\n",
        "    \n",
        "    print(\"üìä Loading CLIP model for evaluation...\")\n",
        "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "    clip_model = clip_model.to(device).eval()\n",
        "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    \n",
        "    def calculate_clip_similarity(image, text):\n",
        "        with torch.no_grad():\n",
        "            image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "            text_input = clip_tokenizer([text]).to(device)\n",
        "            \n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_input)\n",
        "            \n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            \n",
        "            similarity = (image_features @ text_features.T).squeeze().item()\n",
        "            return float(similarity)\n",
        "    \n",
        "    # Evaluate all results\n",
        "    if 'all_results' in locals() and all_results:\n",
        "        print(\"\\\\nüìä CLIP Similarity Analysis:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Group results by model and method\n",
        "        evaluation_summary = {}\n",
        "        \n",
        "        for result in all_results:\n",
        "            model = result['model']\n",
        "            method = result['method']\n",
        "            key = f\"{model}_{method}\"\n",
        "            \n",
        "            if key not in evaluation_summary:\n",
        "                evaluation_summary[key] = []\n",
        "            \n",
        "            # Calculate CLIP similarity with original prompt\n",
        "            similarity = calculate_clip_similarity(result['image'], result['original_prompt'])\n",
        "            evaluation_summary[key].append(similarity)\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"{'Model':<8} {'Method':<15} {'Avg CLIP':<10} {'Std':<8} {'Samples':<8}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for key, scores in evaluation_summary.items():\n",
        "            model, method = key.split('_', 1)\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            \n",
        "            print(f\"{model:<8} {method:<15} {avg_score:.3f}     {std_score:.3f}   {len(scores)}\")\n",
        "        \n",
        "        # Compare methods within each model\n",
        "        print(f\"\\\\nüìà Model Comparison:\")\n",
        "        for model in ['SDXL', 'SD15']:\n",
        "            print(f\"\\\\n{model}:\")\n",
        "            baseline_scores = evaluation_summary.get(f'{model}_baseline', [])\n",
        "            compel_scores = evaluation_summary.get(f'{model}_compel', [])\n",
        "            word_scores = evaluation_summary.get(f'{model}_word_replacement', [])\n",
        "            \n",
        "            if baseline_scores and compel_scores:\n",
        "                compel_improvement = np.mean(compel_scores) - np.mean(baseline_scores)\n",
        "                print(f\"  Compel vs Baseline: {compel_improvement:+.3f}\")\n",
        "            \n",
        "            if baseline_scores and word_scores:\n",
        "                word_improvement = np.mean(word_scores) - np.mean(baseline_scores)\n",
        "                print(f\"  Word Enhancement vs Baseline: {word_improvement:+.3f}\")\n",
        "        \n",
        "        print(\"\\\\n‚úÖ CLIP evaluation completed!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No results to evaluate - run the experiment first!\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  open-clip-torch not installed - skipping CLIP evaluation\")\n",
        "    print(\"   Install with: pip install open-clip-torch\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in CLIP evaluation: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Summary & Next Steps\n",
        "\n",
        "### üìÅ **Generated Files:**\n",
        "- `model_comparison_results/sdxl/p01_baseline.png` - SDXL baseline images\n",
        "- `model_comparison_results/sdxl/p01_compel.png` - SDXL Compel-enhanced images  \n",
        "- `model_comparison_results/sdxl/p01_word_replacement.png` - SDXL word-enhanced images\n",
        "- `model_comparison_results/sd15/p01_baseline.png` - SD1.5 baseline images\n",
        "- `model_comparison_results/sd15/p01_compel.png` - SD1.5 Compel-enhanced images\n",
        "- `model_comparison_results/sd15/p01_word_replacement.png` - SD1.5 word-enhanced images  \n",
        "- `model_comparison_results/comprehensive_results.csv` - **Complete analysis CSV**\n",
        "\n",
        "### üîç **Analysis Questions:**\n",
        "1. **Which model performs better overall?** (SDXL vs SD1.5)\n",
        "2. **Which enhancement method is most effective?** (Compel vs Word Replacement)\n",
        "3. **Are there specific jewelry terms that benefit more from certain enhancements?**\n",
        "4. **Does the engraved 'M' visibility improve with any method?**\n",
        "\n",
        "### üéØ **Key Findings to Look For:**\n",
        "- **Visual quality differences** between models and methods\n",
        "- **CLIP similarity improvements** indicating better prompt adherence\n",
        "- **Specific jewelry features** rendered more accurately\n",
        "- **Style consistency** (modern vs vintage aesthetic)\n",
        "\n",
        "### üí° **Next Steps:**\n",
        "1. **Visual inspection** - Compare generated images side by side\n",
        "2. **CSV analysis** - Use spreadsheet tools for systematic comparison  \n",
        "3. **CLIP scores** - Review quantitative improvements\n",
        "4. **Method selection** - Choose best-performing combination for production\n",
        "5. **Integration** - Implement winning approach in main pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**üèÜ Goal**: Determine the optimal model + enhancement combination for high-quality jewelry image generation!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
