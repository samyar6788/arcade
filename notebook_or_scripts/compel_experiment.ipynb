{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Compel Experiment - SDXL vs SD1.5 Prompt Weighting for Jewelry\n",
        "\n",
        "## Testing Compel Library for Enhanced Prompt Adherence\n",
        "\n",
        "This notebook compares baseline vs Compel-enhanced generation for SDXL and SD1.5.\n",
        "\n",
        "**Key Features:**\n",
        "- Side-by-side image comparison\n",
        "- Jewelry-specific term weighting with `++` syntax\n",
        "- Cross-model comparison (SDXL vs SD1.5)\n",
        "- Quantitative evaluation with CLIP similarity\n",
        "- Special test for engraved 'M' issue\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment for Colab)\n",
        "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device and GPU info\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected - generation will be slow\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"compel_results\", exist_ok=True)\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pipelines and Compel - FIXED VERSION\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline\n",
        "\n",
        "def load_pipeline_and_compel(model_id: str):\n",
        "    \"\"\"Load pipeline and corresponding Compel instance\"\"\"\n",
        "    if \"xl\" in model_id.lower():\n",
        "        # SDXL\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_id, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
        "            text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=[False, True],\n",
        "        )\n",
        "        return pipe, compel_inst, True  # True = is_sdxl\n",
        "    else:\n",
        "        # SD1.5\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
        "        ).to(device)\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=pipe.tokenizer,\n",
        "            text_encoder=pipe.text_encoder,\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=False,\n",
        "        )\n",
        "        return pipe, compel_inst, False  # False = not_sdxl\n",
        "\n",
        "def generate_baseline_and_compel(pipe, compel_inst, is_sdxl, prompt, neg_prompt, w=768, h=768, steps=30, cfg=7.5, seed=42):\n",
        "    \"\"\"Generate baseline and Compel-enhanced images\"\"\"\n",
        "    gen = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    # Baseline generation\n",
        "    img_baseline = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=neg_prompt,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=cfg,\n",
        "        width=w, height=h,\n",
        "        generator=gen,\n",
        "    ).images[0]\n",
        "    \n",
        "    # Reset generator for consistent comparison\n",
        "    gen = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    # Compel generation\n",
        "    if is_sdxl:\n",
        "        # SDXL: dual encoders, returns conditioning and pooled\n",
        "        cond, pooled = compel_inst([prompt, neg_prompt])\n",
        "        img_compel = pipe(\n",
        "            prompt_embeds=cond[0:1], \n",
        "            pooled_prompt_embeds=pooled[0:1],\n",
        "            negative_prompt_embeds=cond[1:2], \n",
        "            negative_pooled_prompt_embeds=pooled[1:2],\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=5.0,  # SDXL typically uses lower CFG\n",
        "            width=w, height=h,\n",
        "            generator=gen,\n",
        "        ).images[0]\n",
        "    else:\n",
        "        # SD1.5: single encoder\n",
        "        pos_cond = compel_inst.build_conditioning_tensor(prompt)\n",
        "        neg_cond = compel_inst.build_conditioning_tensor(neg_prompt)\n",
        "        img_compel = pipe(\n",
        "            prompt_embeds=pos_cond,\n",
        "            negative_prompt_embeds=neg_cond,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            width=w, height=h,\n",
        "            generator=gen,\n",
        "        ).images[0]\n",
        "    \n",
        "    return img_baseline, img_compel\n",
        "\n",
        "print(\"‚úÖ Pipeline loading functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with the problematic 'M' engraving prompt specifically\n",
        "print(\"üîç Testing the engraved 'M' prompt with enhanced weighting...\")\n",
        "\n",
        "# Load SDXL for this test\n",
        "pipe_sdxl, compel_sdxl, is_sdxl = load_pipeline_and_compel(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "\n",
        "# Original signet ring prompt\n",
        "original_prompt = \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\"\n",
        "\n",
        "# Enhanced version with extra focus on the letter M\n",
        "enhanced_prompt = (\n",
        "    \"modern signet++ ring, oval face, engraved++ gothic++ initial++ 'M'++, \"\n",
        "    \"uppercase letter M+++, monogram M+++, blackletter+++, \"\n",
        "    \"deep crisp engraving+++, high-polish sterling silver, subtle reflection, \"\n",
        "    \"macro close-up of face, centered composition, shows letter clearly+++\"\n",
        ")\n",
        "\n",
        "enhanced_negative = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly, no other letters, not blank, no pattern, blurry text\"\n",
        "\n",
        "print(f\"Original: {original_prompt}\")\n",
        "print(f\"Enhanced: {enhanced_prompt}\")\n",
        "print(f\"Negative: {enhanced_negative}\")\n",
        "\n",
        "# Generate comparison\n",
        "img_base, img_compel = generate_baseline_and_compel(\n",
        "    pipe_sdxl, compel_sdxl, is_sdxl, \n",
        "    original_prompt, enhanced_negative,\n",
        "    w=1024, h=1024, steps=30, cfg=5.0, seed=123\n",
        ")\n",
        "\n",
        "# Also test with the enhanced prompt\n",
        "_, img_enhanced = generate_baseline_and_compel(\n",
        "    pipe_sdxl, compel_sdxl, is_sdxl,\n",
        "    enhanced_prompt, enhanced_negative, \n",
        "    w=1024, h=1024, steps=30, cfg=5.0, seed=123\n",
        ")\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "axes[0].imshow(img_base)\n",
        "axes[0].set_title(\"Baseline (Original Prompt)\", fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(img_compel)\n",
        "axes[1].set_title(\"Compel (Original + Weighting)\", fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(img_enhanced)\n",
        "axes[2].set_title(\"Enhanced (Weighted + Extra Terms)\", fontweight='bold')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"compel_results/signet_M_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Engraved 'M' test completed - check if the letter is more visible!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vpeDs_AmyHA"
      },
      "source": [
        "# üß™ Compel Experiment - SDXL Prompt Weighting for Jewelry\n",
        "\n",
        "## Testing Compel Library for Enhanced Prompt Adherence\n",
        "\n",
        "This notebook compares baseline SDXL generation vs Compel-enhanced generation for all 8 test prompts.\n",
        "\n",
        "**Key Features:**\n",
        "- Side-by-side image comparison\n",
        "- Jewelry-specific term weighting with `++` syntax\n",
        "- Quantitative evaluation with CLIP similarity\n",
        "- Export results for analysis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VVkuAh0myHB"
      },
      "source": [
        "## üîß Setup & Installation\n",
        "\n",
        "**For Colab Users:**\n",
        "1. Enable GPU: `Runtime` ‚Üí `Change runtime type` ‚Üí `GPU`\n",
        "2. Install dependencies below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5qn3bnhmyHB"
      },
      "outputs": [],
      "source": [
        "# Install required packages (uncomment for Colab)\n",
        "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch\n",
        "\n",
        "# For local development, ensure you have:\n",
        "# pip install compel>=2.0.0 open-clip-torch\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device and GPU info\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected - generation will be slow\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"compel_results\", exist_ok=True)\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GWecgbPWm-co"
      },
      "outputs": [],
      "source": [
        "!pip install compel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egwAV1dUmyHC"
      },
      "outputs": [],
      "source": [
        "# Load SDXL pipeline and Compel\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading SDXL pipeline...\")\n",
        "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "print(\"üîÑ Initializing Compel...\")\n",
        "compel = Compel(\n",
        "    tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2],\n",
        "    text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2],\n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "    requires_pooled=[False, True]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Pipeline and Compel ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBtReKICmyHC"
      },
      "outputs": [],
      "source": [
        "# Define the 8 test prompts and create Compel-enhanced versions\n",
        "test_prompts = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav√©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "# Create Compel-enhanced versions with ++ weighting for critical jewelry terms\n",
        "def create_compel_prompt(prompt):\n",
        "    \"\"\"Add ++ weighting to critical jewelry terms for Compel\"\"\"\n",
        "    # Critical jewelry terms to emphasize\n",
        "    critical_terms = {\n",
        "        \"channel-set\": \"(channel-set)++\",\n",
        "        \"threader\": \"threader++\",\n",
        "        \"bezel-set\": \"(bezel-set)++\",\n",
        "        \"eternity band\": \"(eternity band)++\",\n",
        "        \"huggie\": \"huggie++\",\n",
        "        \"bypass\": \"bypass++\",\n",
        "        \"pav√©\": \"pav√©++\",\n",
        "        \"signet\": \"signet++\",\n",
        "        \"cuff\": \"cuff++\",\n",
        "        \"cluster\": \"cluster++\",\n",
        "    }\n",
        "\n",
        "    enhanced_prompt = prompt\n",
        "    for term, weighted_term in critical_terms.items():\n",
        "        if term in prompt.lower():\n",
        "            # Replace with case-sensitive match\n",
        "            enhanced_prompt = enhanced_prompt.replace(term, weighted_term)\n",
        "    enhanced_prompt = enhanced_prompt + \"modern++ (photo realistic)+++\"\n",
        "\n",
        "\n",
        "    return enhanced_prompt\n",
        "\n",
        "# Create enhanced prompts\n",
        "compel_prompts = [create_compel_prompt(prompt) for prompt in test_prompts]\n",
        "\n",
        "# Display comparison\n",
        "print(\"üìù Prompt Comparison:\")\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    print(f\"\\n{i}. Original: {original}\")\n",
        "    print(f\"   Enhanced: {enhanced}\")\n",
        "\n",
        "# Common negative prompt\n",
        "negative_prompt = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly\"\n",
        "print(f\"\\n‚ùå Negative prompt: {negative_prompt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMSfrxs6myHC"
      },
      "outputs": [],
      "source": [
        "# Generation functions\n",
        "def generate_baseline(prompt, seed=42):\n",
        "    \"\"\"Generate image using standard SDXL pipeline\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024,\n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "def generate_with_compel(prompt, seed=42):\n",
        "    \"\"\"Generate image using Compel-enhanced embeddings\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "    # Create conditioning with Compel\n",
        "    conditioning, pooled = compel([prompt, negative_prompt])\n",
        "\n",
        "    # Generate image with embeddings\n",
        "    image = pipeline(\n",
        "        prompt_embeds=conditioning[0:1],\n",
        "        pooled_prompt_embeds=pooled[0:1],\n",
        "        negative_prompt_embeds=conditioning[1:2],\n",
        "        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024,\n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "def compare_prompts(original_prompt, compel_prompt, prompt_idx, seed=42):\n",
        "    \"\"\"Generate and compare baseline vs Compel images\"\"\"\n",
        "    print(f\"\\nüé® Generating images for prompt {prompt_idx}...\")\n",
        "    print(f\"Original: {original_prompt[:80]}...\")\n",
        "    print(f\"Compel:   {compel_prompt[:80]}...\")\n",
        "\n",
        "    # Generate both versions\n",
        "    baseline_img = generate_baseline(original_prompt, seed)\n",
        "    compel_img = generate_with_compel(compel_prompt, seed)\n",
        "\n",
        "    # Save images\n",
        "    baseline_img.save(f\"compel_results/prompt_{prompt_idx:02d}_baseline.png\")\n",
        "    compel_img.save(f\"compel_results/prompt_{prompt_idx:02d}_compel.png\")\n",
        "\n",
        "    # Create side-by-side comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    axes[0].imshow(baseline_img)\n",
        "    axes[0].set_title(f\"Baseline (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(compel_img)\n",
        "    axes[1].set_title(f\"Compel Enhanced (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"compel_results/comparison_{prompt_idx:02d}.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return baseline_img, compel_img\n",
        "\n",
        "print(\"‚úÖ Generation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBN0rn_ZmyHC"
      },
      "source": [
        "## üß™ Single Prompt Test\n",
        "\n",
        "Test with one prompt first to verify everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aRi579amyHC"
      },
      "outputs": [],
      "source": [
        "# Test with first prompt\n",
        "test_idx = 1\n",
        "original = test_prompts[0]\n",
        "enhanced = compel_prompts[0]\n",
        "\n",
        "print(f\"üß™ Testing Prompt {test_idx}:\")\n",
        "print(f\"Original: {original}\")\n",
        "print(f\"Enhanced: {enhanced}\")\n",
        "\n",
        "# Generate comparison\n",
        "baseline_img, compel_img = compare_prompts(original, enhanced, test_idx, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC_xSlOQmyHC"
      },
      "source": [
        "## üéØ Generate All 8 Prompts\n",
        "\n",
        "Run this cell to generate all comparisons (will take some time):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq68BN4HmyHC"
      },
      "outputs": [],
      "source": [
        "# Generate all 8 prompts\n",
        "print(\"üöÄ Starting full experiment - generating all 8 prompts...\")\n",
        "print(\"‚è±Ô∏è  This will take approximately 8-16 minutes depending on your GPU\")\n",
        "\n",
        "results = []\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    try:\n",
        "        baseline_img, compel_img = compare_prompts(original, enhanced, i, seed=42+i)\n",
        "        results.append({\n",
        "            'prompt_id': i,\n",
        "            'original_prompt': original,\n",
        "            'enhanced_prompt': enhanced,\n",
        "            'baseline_image': baseline_img,\n",
        "            'compel_image': compel_img\n",
        "        })\n",
        "        print(f\"‚úÖ Prompt {i}/8 completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with prompt {i}: {e}\")\n",
        "\n",
        "print(f\"\\nüéâ Experiment completed! Generated {len(results)}/8 prompt comparisons\")\n",
        "print(f\"üìÅ Results saved in: compel_results/\")\n",
        "print(f\"üìä Check the comparison images to evaluate the differences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRXX0e78myHC"
      },
      "source": [
        "## üìä Quantitative Evaluation (Optional)\n",
        "\n",
        "Add CLIP similarity scoring to measure prompt adherence quantitatively:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bclLwJ6kmyHC"
      },
      "outputs": [],
      "source": [
        "# CLIP Evaluation (uncomment if open-clip-torch is installed)\n",
        "try:\n",
        "    import open_clip\n",
        "\n",
        "    # Ensure device is defined\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load CLIP model for evaluation\n",
        "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "    clip_model = clip_model.to(device).eval()\n",
        "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "    def calculate_clip_similarity(image, text):\n",
        "        \"\"\"Calculate CLIP similarity between image and text\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Preprocess image and text\n",
        "            image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "            text_input = clip_tokenizer([text])\n",
        "\n",
        "            # Get embeddings\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_input.to(device))\n",
        "\n",
        "            # Normalize and calculate cosine similarity\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = (image_features @ text_features.T).squeeze().item()\n",
        "\n",
        "            return similarity\n",
        "\n",
        "    # Evaluate results if we have them\n",
        "    if 'results' in locals() and results:\n",
        "        print(\"üìä CLIP Similarity Evaluation:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        baseline_scores = []\n",
        "        compel_scores = []\n",
        "\n",
        "        for result in results:\n",
        "            prompt_id = result['prompt_id']\n",
        "            original_prompt = result['original_prompt']\n",
        "\n",
        "            # Calculate similarities\n",
        "            baseline_sim = calculate_clip_similarity(result['baseline_image'], original_prompt)\n",
        "            compel_sim = calculate_clip_similarity(result['compel_image'], original_prompt)\n",
        "\n",
        "            baseline_scores.append(baseline_sim)\n",
        "            compel_scores.append(compel_sim)\n",
        "\n",
        "            improvement = compel_sim - baseline_sim\n",
        "            print(f\"Prompt {prompt_id:2d}: Baseline={baseline_sim:.3f}, Compel={compel_sim:.3f}, Œî={improvement:+.3f}\")\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_baseline = sum(baseline_scores) / len(baseline_scores)\n",
        "        avg_compel = sum(compel_scores) / len(compel_scores)\n",
        "        avg_improvement = avg_compel - avg_baseline\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"üìà Average Scores:\")\n",
        "        print(f\"   Baseline:    {avg_baseline:.3f}\")\n",
        "        print(f\"   Compel:      {avg_compel:.3f}\")\n",
        "        print(f\"   Improvement: {avg_improvement:+.3f} ({avg_improvement/avg_baseline*100:+.1f}%)\")\n",
        "\n",
        "        if avg_improvement > 0:\n",
        "            print(\"üéâ Compel shows improvement in prompt adherence!\")\n",
        "        else:\n",
        "            print(\"üìù Baseline performs better - consider adjusting weighting strategy\")\n",
        "\n",
        "    print(\"‚úÖ CLIP evaluation available\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  open-clip-torch not installed - skipping quantitative evaluation\")\n",
        "    print(\"   Install with: pip install open-clip-torch\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in CLIP evaluation: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYyIzz9hmyHD"
      },
      "source": [
        "## üìä Export Prompt Comparison CSV\n",
        "\n",
        "Generate a CSV file with original vs Compel-enhanced prompts for analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAmrvqx6myHD"
      },
      "outputs": [],
      "source": [
        "# Export prompt comparison to CSV\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "def export_prompt_comparison():\n",
        "    \"\"\"Export original and Compel-enhanced prompts to CSV\"\"\"\n",
        "\n",
        "    # Create comparison data\n",
        "    comparison_data = []\n",
        "    for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "        comparison_data.append({\n",
        "            'prompt_id': i,\n",
        "            'original_prompt': original,\n",
        "            'compel_enhanced_prompt': enhanced,\n",
        "            'changes': ', '.join([term + '++' for term in ['channel-set', 'threader', 'bezel-set', 'eternity band', 'huggie', 'bypass', 'pav√©', 'signet', 'cuff', 'cluster', 'diamond', 'sapphire', 'gold', 'platinum'] if term in original.lower()])\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_filename = \"compel_results/prompt_comparison.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Display the comparison\n",
        "    print(\"üìä Prompt Comparison Table:\")\n",
        "    print(\"=\" * 100)\n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"\\nüî¢ Prompt {row['prompt_id']}:\")\n",
        "        print(f\"   Original: {row['original_prompt'][:80]}...\")\n",
        "        print(f\"   Enhanced: {row['compel_enhanced_prompt'][:80]}...\")\n",
        "        if row['changes']:\n",
        "            print(f\"   Weighted: {row['changes']}\")\n",
        "\n",
        "    print(f\"\\nüíæ CSV saved to: {csv_filename}\")\n",
        "    print(f\"üìã Total prompts: {len(df)}\")\n",
        "\n",
        "    # Show summary of changes\n",
        "    all_changes = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row['changes']:\n",
        "            all_changes.extend(row['changes'].split(', '))\n",
        "\n",
        "    from collections import Counter\n",
        "    change_counts = Counter(all_changes)\n",
        "\n",
        "    print(f\"\\nüìà Most weighted terms:\")\n",
        "    for term, count in change_counts.most_common(5):\n",
        "        print(f\"   {term}: {count} times\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Run the export\n",
        "prompt_df = export_prompt_comparison()\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nüìã Preview of CSV data:\")\n",
        "print(prompt_df[['prompt_id', 'original_prompt', 'compel_enhanced_prompt']].head(3).to_string(max_colwidth=50))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJWVkNsQmyHD"
      },
      "source": [
        "## üìã Results Summary\n",
        "\n",
        "The notebook generates:\n",
        "\n",
        "### üìÅ **Files Created:**\n",
        "- `compel_results/prompt_XX_baseline.png` - Baseline generations\n",
        "- `compel_results/prompt_XX_compel.png` - Compel-enhanced generations  \n",
        "- `compel_results/comparison_XX.png` - Side-by-side comparisons\n",
        "- `compel_results/prompt_comparison.csv` - CSV with original vs enhanced prompts\n",
        "\n",
        "### üîç **What to Look For:**\n",
        "\n",
        "**Visual Differences:**\n",
        "- **Prompt adherence**: Does Compel better capture specific jewelry terms?\n",
        "- **Detail quality**: Are jewelry features more defined/accurate?\n",
        "- **Style consistency**: Modern vs vintage aesthetic differences\n",
        "\n",
        "**Quantitative Metrics:**\n",
        "- **CLIP similarity scores**: Higher = better prompt adherence\n",
        "- **Average improvement**: Overall lift from Compel weighting\n",
        "\n",
        "### üéØ **Next Steps:**\n",
        "1. **Visual inspection**: Compare side-by-side images\n",
        "2. **Quantitative analysis**: Review CLIP similarity scores\n",
        "3. **CSV analysis**: Use the exported CSV for systematic comparison\n",
        "4. **Fine-tuning**: Adjust `++` weights based on results\n",
        "5. **Integration**: If successful, integrate into main pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Pro Tip:** Try different weighting levels (`+`, `++`, `+++`) for terms that show the most improvement!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoo6tpOYmyHD"
      },
      "source": [
        "## üî¨ Compare SDXL vs SD 1.5 with and without Compel\n",
        "\n",
        "This section loads each model (SDXL and SD 1.5), generates baseline and Compel-enhanced images for all 8 prompts at 768√ó768, and optionally computes CLIP adherence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u-QpHcPmyHD"
      },
      "outputs": [],
      "source": [
        "# Utilities for loading pipelines with/without Compel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "class ModelVariant:\n",
        "    SD15 = \"runwayml/stable-diffusion-v1-5\"\n",
        "    SDXL = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "def load_pipeline(model_id: str, device: str = device):\n",
        "    if \"xl\" in model_id.lower():\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_id, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
        "            text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=[False, True],\n",
        "        )\n",
        "        return pipe, compel_inst\n",
        "    else:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
        "        ).to(device)\n",
        "        # For SD1.5 we can still use Compel with single tokenizer/encoder\n",
        "        compel_inst = Compel(\n",
        "            tokenizer=pipe.tokenizer,\n",
        "            text_encoder=pipe.text_encoder,\n",
        "            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "            requires_pooled=False,\n",
        "        )\n",
        "        return pipe, compel_inst\n",
        "\n",
        "\n",
        "def generate_pair(pipe, compel_inst, base_prompt: str, neg_prompt: str, w: int=768, h: int=768, steps: int=30, cfg: float=7.5, seed: int=42):\n",
        "    gen = torch.Generator(device=device).manual_seed(seed)\n",
        "    # Baseline\n",
        "    img_base = pipe(\n",
        "        prompt=base_prompt,\n",
        "        negative_prompt=neg_prompt,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=cfg,\n",
        "        width=w, height=h,\n",
        "        generator=gen,\n",
        "    ).images[0]\n",
        "    # Compel\n",
        "    if isinstance(compel_inst.tokenizer, list):\n",
        "        cond, pooled = compel_inst([base_prompt, neg_prompt])\n",
        "        img_compel = pipe(\n",
        "            prompt_embeds=cond[0:1], pooled_prompt_embeds=pooled[0:1],\n",
        "            negative_prompt_embeds=cond[1:2], negative_pooled_prompt_embeds=pooled[1:2],\n",
        "            num_inference_steps=steps, guidance_scale=(5.0 if \"xl\" in pipe.__class__.__name__.lower() else cfg),\n",
        "            width=w, height=h, generator=gen,\n",
        "        ).images[0]\n",
        "    else:\n",
        "        cond = compel_inst.build_conditioning_tensor(base_prompt)\n",
        "        ncond = compel_inst.build_conditioning_tensor(neg_prompt)\n",
        "        img_compel = pipe(\n",
        "            prompt_embeds=cond, negative_prompt_embeds=ncond,\n",
        "            num_inference_steps=steps, guidance_scale=cfg,\n",
        "            width=w, height=h, generator=gen,\n",
        "        ).images[0]\n",
        "    return img_base, img_compel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOCLrvpGmyHD"
      },
      "outputs": [],
      "source": [
        "# Run comparison for SDXL and SD 1.5\n",
        "models = [ModelVariant.SDXL, ModelVariant.SD15]\n",
        "results_compare = {}\n",
        "\n",
        "for mid in models:\n",
        "    print(f\"\\nüß™ Testing model: {mid}\")\n",
        "    pipe, compel_inst = load_pipeline(mid)\n",
        "    model_key = 'SDXL' if 'xl' in mid.lower() else 'SD15'\n",
        "    results_compare[model_key] = []\n",
        "\n",
        "    for i, (orig, enh) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "        try:\n",
        "            img_b, img_c = generate_pair(pipe, compel_inst, orig, negative_prompt, w=768, h=768, steps=28, cfg=(5.0 if model_key=='SDXL' else 7.5), seed=100+i)\n",
        "            # Save\n",
        "            base_path = f\"compel_results/{model_key.lower()}\"\n",
        "            os.makedirs(base_path, exist_ok=True)\n",
        "            img_b.save(f\"{base_path}/p{i:02d}_baseline.png\")\n",
        "            img_c.save(f\"{base_path}/p{i:02d}_compel.png\")\n",
        "            results_compare[model_key].append((img_b, img_c, orig))\n",
        "            print(f\"  ‚úÖ Prompt {i} done\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Prompt {i} failed: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Finished generating for both models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ4JQiQ2myHD"
      },
      "outputs": [],
      "source": [
        "# Optional: CLIP evaluation across models if open_clip is available\n",
        "try:\n",
        "    import open_clip\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "    clip_model = clip_model.to(device).eval()\n",
        "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "    def clip_score(image, text):\n",
        "        with torch.no_grad():\n",
        "            image_in = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "            text_in = clip_tokenizer([text]).to(device)\n",
        "            im = clip_model.encode_image(image_in)\n",
        "            tx = clip_model.encode_text(text_in)\n",
        "            im = im / im.norm(dim=-1, keepdim=True)\n",
        "            tx = tx / tx.norm(dim=-1, keepdim=True)\n",
        "            return float((im @ tx.T).squeeze().item())\n",
        "\n",
        "    for model_key, rows in results_compare.items():\n",
        "        print(f\"\\nüìä CLIP Adherence for {model_key}\")\n",
        "        base_scores, compel_scores = [], []\n",
        "        for (img_b, img_c, text) in rows:\n",
        "            base_scores.append(clip_score(img_b, text))\n",
        "            compel_scores.append(clip_score(img_c, text))\n",
        "        if base_scores and compel_scores:\n",
        "            mb, mc = sum(base_scores)/len(base_scores), sum(compel_scores)/len(compel_scores)\n",
        "            print(f\"   Baseline: {mb:.3f}  Compel: {mc:.3f}  Œî={mc-mb:+.3f}\")\n",
        "        else:\n",
        "            print(\"   No results to score.\")\n",
        "    print(\"\\n‚úÖ Cross-model CLIP evaluation complete\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Skipping cross-model CLIP evaluation: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
