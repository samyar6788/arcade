{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§ª Compel Experiment - SDXL Prompt Weighting for Jewelry\n",
        "\n",
        "## Testing Compel Library for Enhanced Prompt Adherence\n",
        "\n",
        "This notebook compares baseline SDXL generation vs Compel-enhanced generation for all 8 test prompts.\n",
        "\n",
        "**Key Features:**\n",
        "- Side-by-side image comparison\n",
        "- Jewelry-specific term weighting with `++` syntax\n",
        "- Quantitative evaluation with CLIP similarity\n",
        "- Export results for analysis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Setup & Installation\n",
        "\n",
        "**For Colab Users:**\n",
        "1. Enable GPU: `Runtime` â†’ `Change runtime type` â†’ `GPU`\n",
        "2. Install dependencies below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment for Colab)\n",
        "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch\n",
        "\n",
        "# For local development, ensure you have:\n",
        "# pip install compel>=2.0.0 open-clip-torch\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device and GPU info\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected - generation will be slow\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"compel_results\", exist_ok=True)\n",
        "print(\"âœ… Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SDXL pipeline and Compel\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ”„ Loading SDXL pipeline...\")\n",
        "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
        "    variant=\"fp16\", \n",
        "    use_safetensors=True, \n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "print(\"ğŸ”„ Initializing Compel...\")\n",
        "compel = Compel(\n",
        "    tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], \n",
        "    text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], \n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, \n",
        "    requires_pooled=[False, True]\n",
        ")\n",
        "\n",
        "print(\"âœ… Pipeline and Compel ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 8 test prompts and create Compel-enhanced versions\n",
        "test_prompts = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pavÃ©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "# Create Compel-enhanced versions with ++ weighting for critical jewelry terms\n",
        "def create_compel_prompt(prompt):\n",
        "    \"\"\"Add ++ weighting to critical jewelry terms for Compel\"\"\"\n",
        "    # Critical jewelry terms to emphasize\n",
        "    critical_terms = {\n",
        "        \"channel-set\": \"channel-set++\",\n",
        "        \"threader\": \"threader++\", \n",
        "        \"bezel-set\": \"bezel-set++\",\n",
        "        \"eternity band\": \"eternity band++\",\n",
        "        \"huggie\": \"huggie++\",\n",
        "        \"bypass\": \"bypass++\",\n",
        "        \"pavÃ©\": \"pavÃ©++\",\n",
        "        \"signet\": \"signet++\",\n",
        "        \"cuff\": \"cuff++\",\n",
        "        \"cluster\": \"cluster++\",\n",
        "        \"diamond\": \"diamond++\",\n",
        "        \"sapphire\": \"sapphire++\",\n",
        "        \"gold\": \"gold++\",\n",
        "        \"platinum\": \"platinum++\"\n",
        "    }\n",
        "    \n",
        "    enhanced_prompt = prompt\n",
        "    for term, weighted_term in critical_terms.items():\n",
        "        if term in prompt.lower():\n",
        "            # Replace with case-sensitive match\n",
        "            enhanced_prompt = enhanced_prompt.replace(term, weighted_term)\n",
        "    \n",
        "    return enhanced_prompt\n",
        "\n",
        "# Create enhanced prompts\n",
        "compel_prompts = [create_compel_prompt(prompt) for prompt in test_prompts]\n",
        "\n",
        "# Display comparison\n",
        "print(\"ğŸ“ Prompt Comparison:\")\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    print(f\"\\n{i}. Original: {original}\")\n",
        "    print(f\"   Enhanced: {enhanced}\")\n",
        "\n",
        "# Common negative prompt\n",
        "negative_prompt = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly\"\n",
        "print(f\"\\nâŒ Negative prompt: {negative_prompt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation functions\n",
        "def generate_baseline(prompt, seed=42):\n",
        "    \"\"\"Generate image using standard SDXL pipeline\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024,\n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "def generate_with_compel(prompt, seed=42):\n",
        "    \"\"\"Generate image using Compel-enhanced embeddings\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    # Create conditioning with Compel\n",
        "    conditioning, pooled = compel([prompt, negative_prompt])\n",
        "    \n",
        "    # Generate image with embeddings\n",
        "    image = pipeline(\n",
        "        prompt_embeds=conditioning[0:1], \n",
        "        pooled_prompt_embeds=pooled[0:1], \n",
        "        negative_prompt_embeds=conditioning[1:2], \n",
        "        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024, \n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "def compare_prompts(original_prompt, compel_prompt, prompt_idx, seed=42):\n",
        "    \"\"\"Generate and compare baseline vs Compel images\"\"\"\n",
        "    print(f\"\\nğŸ¨ Generating images for prompt {prompt_idx}...\")\n",
        "    print(f\"Original: {original_prompt[:80]}...\")\n",
        "    print(f\"Compel:   {compel_prompt[:80]}...\")\n",
        "    \n",
        "    # Generate both versions\n",
        "    baseline_img = generate_baseline(original_prompt, seed)\n",
        "    compel_img = generate_with_compel(compel_prompt, seed)\n",
        "    \n",
        "    # Save images\n",
        "    baseline_img.save(f\"compel_results/prompt_{prompt_idx:02d}_baseline.png\")\n",
        "    compel_img.save(f\"compel_results/prompt_{prompt_idx:02d}_compel.png\")\n",
        "    \n",
        "    # Create side-by-side comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    axes[0].imshow(baseline_img)\n",
        "    axes[0].set_title(f\"Baseline (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(compel_img)\n",
        "    axes[1].set_title(f\"Compel Enhanced (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"compel_results/comparison_{prompt_idx:02d}.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return baseline_img, compel_img\n",
        "\n",
        "print(\"âœ… Generation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª Single Prompt Test\n",
        "\n",
        "Test with one prompt first to verify everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with first prompt\n",
        "test_idx = 1\n",
        "original = test_prompts[0]\n",
        "enhanced = compel_prompts[0]\n",
        "\n",
        "print(f\"ğŸ§ª Testing Prompt {test_idx}:\")\n",
        "print(f\"Original: {original}\")\n",
        "print(f\"Enhanced: {enhanced}\")\n",
        "\n",
        "# Generate comparison\n",
        "baseline_img, compel_img = compare_prompts(original, enhanced, test_idx, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Generate All 8 Prompts\n",
        "\n",
        "Run this cell to generate all comparisons (will take some time):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all 8 prompts\n",
        "print(\"ğŸš€ Starting full experiment - generating all 8 prompts...\")\n",
        "print(\"â±ï¸  This will take approximately 8-16 minutes depending on your GPU\")\n",
        "\n",
        "results = []\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    try:\n",
        "        baseline_img, compel_img = compare_prompts(original, enhanced, i, seed=42+i)\n",
        "        results.append({\n",
        "            'prompt_id': i,\n",
        "            'original_prompt': original,\n",
        "            'enhanced_prompt': enhanced,\n",
        "            'baseline_image': baseline_img,\n",
        "            'compel_image': compel_img\n",
        "        })\n",
        "        print(f\"âœ… Prompt {i}/8 completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error with prompt {i}: {e}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Experiment completed! Generated {len(results)}/8 prompt comparisons\")\n",
        "print(f\"ğŸ“ Results saved in: compel_results/\")\n",
        "print(f\"ğŸ“Š Check the comparison images to evaluate the differences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Quantitative Evaluation (Optional)\n",
        "\n",
        "Add CLIP similarity scoring to measure prompt adherence quantitatively:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP Evaluation (uncomment if open-clip-torch is installed)\n",
        "try:\n",
        "    import open_clip\n",
        "    \n",
        "    # Ensure device is defined\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Load CLIP model for evaluation\n",
        "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "    clip_model = clip_model.to(device).eval()\n",
        "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    \n",
        "    def calculate_clip_similarity(image, text):\n",
        "        \"\"\"Calculate CLIP similarity between image and text\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Preprocess image and text\n",
        "            image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "            text_input = clip_tokenizer([text])\n",
        "            \n",
        "            # Get embeddings\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_input.to(device))\n",
        "            \n",
        "            # Normalize and calculate cosine similarity\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = (image_features @ text_features.T).squeeze().item()\n",
        "            \n",
        "            return similarity\n",
        "    \n",
        "    # Evaluate results if we have them\n",
        "    if 'results' in locals() and results:\n",
        "        print(\"ğŸ“Š CLIP Similarity Evaluation:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        baseline_scores = []\n",
        "        compel_scores = []\n",
        "        \n",
        "        for result in results:\n",
        "            prompt_id = result['prompt_id']\n",
        "            original_prompt = result['original_prompt']\n",
        "            \n",
        "            # Calculate similarities\n",
        "            baseline_sim = calculate_clip_similarity(result['baseline_image'], original_prompt)\n",
        "            compel_sim = calculate_clip_similarity(result['compel_image'], original_prompt)\n",
        "            \n",
        "            baseline_scores.append(baseline_sim)\n",
        "            compel_scores.append(compel_sim)\n",
        "            \n",
        "            improvement = compel_sim - baseline_sim\n",
        "            print(f\"Prompt {prompt_id:2d}: Baseline={baseline_sim:.3f}, Compel={compel_sim:.3f}, Î”={improvement:+.3f}\")\n",
        "        \n",
        "        # Calculate averages\n",
        "        avg_baseline = sum(baseline_scores) / len(baseline_scores)\n",
        "        avg_compel = sum(compel_scores) / len(compel_scores)\n",
        "        avg_improvement = avg_compel - avg_baseline\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "        print(f\"ğŸ“ˆ Average Scores:\")\n",
        "        print(f\"   Baseline:    {avg_baseline:.3f}\")\n",
        "        print(f\"   Compel:      {avg_compel:.3f}\")\n",
        "        print(f\"   Improvement: {avg_improvement:+.3f} ({avg_improvement/avg_baseline*100:+.1f}%)\")\n",
        "        \n",
        "        if avg_improvement > 0:\n",
        "            print(\"ğŸ‰ Compel shows improvement in prompt adherence!\")\n",
        "        else:\n",
        "            print(\"ğŸ“ Baseline performs better - consider adjusting weighting strategy\")\n",
        "    \n",
        "    print(\"âœ… CLIP evaluation available\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"âš ï¸  open-clip-torch not installed - skipping quantitative evaluation\")\n",
        "    print(\"   Install with: pip install open-clip-torch\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error in CLIP evaluation: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ Results Summary\n",
        "\n",
        "The notebook generates:\n",
        "\n",
        "### ğŸ“ **Files Created:**\n",
        "- `compel_results/prompt_XX_baseline.png` - Baseline generations\n",
        "- `compel_results/prompt_XX_compel.png` - Compel-enhanced generations  \n",
        "- `compel_results/comparison_XX.png` - Side-by-side comparisons\n",
        "\n",
        "### ğŸ” **What to Look For:**\n",
        "\n",
        "**Visual Differences:**\n",
        "- **Prompt adherence**: Does Compel better capture specific jewelry terms?\n",
        "- **Detail quality**: Are jewelry features more defined/accurate?\n",
        "- **Style consistency**: Modern vs vintage aesthetic differences\n",
        "\n",
        "**Quantitative Metrics:**\n",
        "- **CLIP similarity scores**: Higher = better prompt adherence\n",
        "- **Average improvement**: Overall lift from Compel weighting\n",
        "\n",
        "### ğŸ¯ **Next Steps:**\n",
        "1. **Visual inspection**: Compare side-by-side images\n",
        "2. **Quantitative analysis**: Review CLIP similarity scores\n",
        "3. **Fine-tuning**: Adjust `++` weights based on results\n",
        "4. **Integration**: If successful, integrate into main pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¡ Pro Tip:** Try different weighting levels (`+`, `++`, `+++`) for terms that show the most improvement!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
