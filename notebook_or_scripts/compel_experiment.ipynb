{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Compel Experiment - SDXL Prompt Weighting for Jewelry\n",
        "\n",
        "## Testing Compel Library for Enhanced Prompt Adherence\n",
        "\n",
        "This notebook compares baseline SDXL generation vs Compel-enhanced generation for all 8 test prompts.\n",
        "\n",
        "**Key Features:**\n",
        "- Side-by-side image comparison\n",
        "- Jewelry-specific term weighting with `++` syntax\n",
        "- Quantitative evaluation with CLIP similarity\n",
        "- Export results for analysis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Installation\n",
        "\n",
        "**For Colab Users:**\n",
        "1. Enable GPU: `Runtime` ‚Üí `Change runtime type` ‚Üí `GPU`\n",
        "2. Install dependencies below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment for Colab)\n",
        "# %pip install torch torchvision diffusers transformers accelerate compel pillow matplotlib open-clip-torch\n",
        "\n",
        "# For local development, ensure you have:\n",
        "# pip install compel>=2.0.0 open-clip-torch\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device and GPU info\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected - generation will be slow\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"compel_results\", exist_ok=True)\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SDXL pipeline and Compel\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading SDXL pipeline...\")\n",
        "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
        "    variant=\"fp16\", \n",
        "    use_safetensors=True, \n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "print(\"üîÑ Initializing Compel...\")\n",
        "compel = Compel(\n",
        "    tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], \n",
        "    text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], \n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, \n",
        "    requires_pooled=[False, True]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Pipeline and Compel ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 8 test prompts and create Compel-enhanced versions\n",
        "test_prompts = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pav√©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "# Create Compel-enhanced versions with ++ weighting for critical jewelry terms\n",
        "def create_compel_prompt(prompt):\n",
        "    \"\"\"Add ++ weighting to critical jewelry terms for Compel\"\"\"\n",
        "    # Critical jewelry terms to emphasize\n",
        "    critical_terms = {\n",
        "        \"channel-set\": \"channel-set++\",\n",
        "        \"threader\": \"threader++\", \n",
        "        \"bezel-set\": \"bezel-set++\",\n",
        "        \"eternity band\": \"eternity band++\",\n",
        "        \"huggie\": \"huggie++\",\n",
        "        \"bypass\": \"bypass++\",\n",
        "        \"pav√©\": \"pav√©++\",\n",
        "        \"signet\": \"signet++\",\n",
        "        \"cuff\": \"cuff++\",\n",
        "        \"cluster\": \"cluster++\",\n",
        "        \"diamond\": \"diamond++\",\n",
        "        \"sapphire\": \"sapphire++\",\n",
        "        \"gold\": \"gold++\",\n",
        "        \"platinum\": \"platinum++\"\n",
        "    }\n",
        "    \n",
        "    enhanced_prompt = prompt\n",
        "    for term, weighted_term in critical_terms.items():\n",
        "        if term in prompt.lower():\n",
        "            # Replace with case-sensitive match\n",
        "            enhanced_prompt = enhanced_prompt.replace(term, weighted_term)\n",
        "    \n",
        "    return enhanced_prompt\n",
        "\n",
        "# Create enhanced prompts\n",
        "compel_prompts = [create_compel_prompt(prompt) for prompt in test_prompts]\n",
        "\n",
        "# Display comparison\n",
        "print(\"üìù Prompt Comparison:\")\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    print(f\"\\n{i}. Original: {original}\")\n",
        "    print(f\"   Enhanced: {enhanced}\")\n",
        "\n",
        "# Common negative prompt\n",
        "negative_prompt = \"vintage, ornate, fussy, cheap, low quality, blurry, deformed, ugly\"\n",
        "print(f\"\\n‚ùå Negative prompt: {negative_prompt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation functions\n",
        "def generate_baseline(prompt, seed=42):\n",
        "    \"\"\"Generate image using standard SDXL pipeline\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024,\n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "def generate_with_compel(prompt, seed=42):\n",
        "    \"\"\"Generate image using Compel-enhanced embeddings\"\"\"\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    \n",
        "    # Create conditioning with Compel\n",
        "    conditioning, pooled = compel([prompt, negative_prompt])\n",
        "    \n",
        "    # Generate image with embeddings\n",
        "    image = pipeline(\n",
        "        prompt_embeds=conditioning[0:1], \n",
        "        pooled_prompt_embeds=pooled[0:1], \n",
        "        negative_prompt_embeds=conditioning[1:2], \n",
        "        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=5.0,\n",
        "        width=1024, \n",
        "        height=1024,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "    \n",
        "    return image\n",
        "\n",
        "def compare_prompts(original_prompt, compel_prompt, prompt_idx, seed=42):\n",
        "    \"\"\"Generate and compare baseline vs Compel images\"\"\"\n",
        "    print(f\"\\nüé® Generating images for prompt {prompt_idx}...\")\n",
        "    print(f\"Original: {original_prompt[:80]}...\")\n",
        "    print(f\"Compel:   {compel_prompt[:80]}...\")\n",
        "    \n",
        "    # Generate both versions\n",
        "    baseline_img = generate_baseline(original_prompt, seed)\n",
        "    compel_img = generate_with_compel(compel_prompt, seed)\n",
        "    \n",
        "    # Save images\n",
        "    baseline_img.save(f\"compel_results/prompt_{prompt_idx:02d}_baseline.png\")\n",
        "    compel_img.save(f\"compel_results/prompt_{prompt_idx:02d}_compel.png\")\n",
        "    \n",
        "    # Create side-by-side comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    axes[0].imshow(baseline_img)\n",
        "    axes[0].set_title(f\"Baseline (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(compel_img)\n",
        "    axes[1].set_title(f\"Compel Enhanced (Prompt {prompt_idx})\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"compel_results/comparison_{prompt_idx:02d}.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return baseline_img, compel_img\n",
        "\n",
        "print(\"‚úÖ Generation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Single Prompt Test\n",
        "\n",
        "Test with one prompt first to verify everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with first prompt\n",
        "test_idx = 1\n",
        "original = test_prompts[0]\n",
        "enhanced = compel_prompts[0]\n",
        "\n",
        "print(f\"üß™ Testing Prompt {test_idx}:\")\n",
        "print(f\"Original: {original}\")\n",
        "print(f\"Enhanced: {enhanced}\")\n",
        "\n",
        "# Generate comparison\n",
        "baseline_img, compel_img = compare_prompts(original, enhanced, test_idx, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Generate All 8 Prompts\n",
        "\n",
        "Run this cell to generate all comparisons (will take some time):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all 8 prompts\n",
        "print(\"üöÄ Starting full experiment - generating all 8 prompts...\")\n",
        "print(\"‚è±Ô∏è  This will take approximately 8-16 minutes depending on your GPU\")\n",
        "\n",
        "results = []\n",
        "for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "    try:\n",
        "        baseline_img, compel_img = compare_prompts(original, enhanced, i, seed=42+i)\n",
        "        results.append({\n",
        "            'prompt_id': i,\n",
        "            'original_prompt': original,\n",
        "            'enhanced_prompt': enhanced,\n",
        "            'baseline_image': baseline_img,\n",
        "            'compel_image': compel_img\n",
        "        })\n",
        "        print(f\"‚úÖ Prompt {i}/8 completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with prompt {i}: {e}\")\n",
        "\n",
        "print(f\"\\nüéâ Experiment completed! Generated {len(results)}/8 prompt comparisons\")\n",
        "print(f\"üìÅ Results saved in: compel_results/\")\n",
        "print(f\"üìä Check the comparison images to evaluate the differences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Quantitative Evaluation (Optional)\n",
        "\n",
        "Add CLIP similarity scoring to measure prompt adherence quantitatively:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP Evaluation (uncomment if open-clip-torch is installed)\n",
        "try:\n",
        "    import open_clip\n",
        "    \n",
        "    # Ensure device is defined\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Load CLIP model for evaluation\n",
        "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "    clip_model = clip_model.to(device).eval()\n",
        "    clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    \n",
        "    def calculate_clip_similarity(image, text):\n",
        "        \"\"\"Calculate CLIP similarity between image and text\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Preprocess image and text\n",
        "            image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "            text_input = clip_tokenizer([text])\n",
        "            \n",
        "            # Get embeddings\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_input.to(device))\n",
        "            \n",
        "            # Normalize and calculate cosine similarity\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = (image_features @ text_features.T).squeeze().item()\n",
        "            \n",
        "            return similarity\n",
        "    \n",
        "    # Evaluate results if we have them\n",
        "    if 'results' in locals() and results:\n",
        "        print(\"üìä CLIP Similarity Evaluation:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        baseline_scores = []\n",
        "        compel_scores = []\n",
        "        \n",
        "        for result in results:\n",
        "            prompt_id = result['prompt_id']\n",
        "            original_prompt = result['original_prompt']\n",
        "            \n",
        "            # Calculate similarities\n",
        "            baseline_sim = calculate_clip_similarity(result['baseline_image'], original_prompt)\n",
        "            compel_sim = calculate_clip_similarity(result['compel_image'], original_prompt)\n",
        "            \n",
        "            baseline_scores.append(baseline_sim)\n",
        "            compel_scores.append(compel_sim)\n",
        "            \n",
        "            improvement = compel_sim - baseline_sim\n",
        "            print(f\"Prompt {prompt_id:2d}: Baseline={baseline_sim:.3f}, Compel={compel_sim:.3f}, Œî={improvement:+.3f}\")\n",
        "        \n",
        "        # Calculate averages\n",
        "        avg_baseline = sum(baseline_scores) / len(baseline_scores)\n",
        "        avg_compel = sum(compel_scores) / len(compel_scores)\n",
        "        avg_improvement = avg_compel - avg_baseline\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "        print(f\"üìà Average Scores:\")\n",
        "        print(f\"   Baseline:    {avg_baseline:.3f}\")\n",
        "        print(f\"   Compel:      {avg_compel:.3f}\")\n",
        "        print(f\"   Improvement: {avg_improvement:+.3f} ({avg_improvement/avg_baseline*100:+.1f}%)\")\n",
        "        \n",
        "        if avg_improvement > 0:\n",
        "            print(\"üéâ Compel shows improvement in prompt adherence!\")\n",
        "        else:\n",
        "            print(\"üìù Baseline performs better - consider adjusting weighting strategy\")\n",
        "    \n",
        "    print(\"‚úÖ CLIP evaluation available\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  open-clip-torch not installed - skipping quantitative evaluation\")\n",
        "    print(\"   Install with: pip install open-clip-torch\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in CLIP evaluation: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Export Prompt Comparison CSV\n",
        "\n",
        "Generate a CSV file with original vs Compel-enhanced prompts for analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export prompt comparison to CSV\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "def export_prompt_comparison():\n",
        "    \"\"\"Export original and Compel-enhanced prompts to CSV\"\"\"\n",
        "    \n",
        "    # Create comparison data\n",
        "    comparison_data = []\n",
        "    for i, (original, enhanced) in enumerate(zip(test_prompts, compel_prompts), 1):\n",
        "        comparison_data.append({\n",
        "            'prompt_id': i,\n",
        "            'original_prompt': original,\n",
        "            'compel_enhanced_prompt': enhanced,\n",
        "            'changes': ', '.join([term + '++' for term in ['channel-set', 'threader', 'bezel-set', 'eternity band', 'huggie', 'bypass', 'pav√©', 'signet', 'cuff', 'cluster', 'diamond', 'sapphire', 'gold', 'platinum'] if term in original.lower()])\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Save to CSV\n",
        "    csv_filename = \"compel_results/prompt_comparison.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    \n",
        "    # Display the comparison\n",
        "    print(\"üìä Prompt Comparison Table:\")\n",
        "    print(\"=\" * 100)\n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"\\nüî¢ Prompt {row['prompt_id']}:\")\n",
        "        print(f\"   Original: {row['original_prompt'][:80]}...\")\n",
        "        print(f\"   Enhanced: {row['compel_enhanced_prompt'][:80]}...\")\n",
        "        if row['changes']:\n",
        "            print(f\"   Weighted: {row['changes']}\")\n",
        "    \n",
        "    print(f\"\\nüíæ CSV saved to: {csv_filename}\")\n",
        "    print(f\"üìã Total prompts: {len(df)}\")\n",
        "    \n",
        "    # Show summary of changes\n",
        "    all_changes = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row['changes']:\n",
        "            all_changes.extend(row['changes'].split(', '))\n",
        "    \n",
        "    from collections import Counter\n",
        "    change_counts = Counter(all_changes)\n",
        "    \n",
        "    print(f\"\\nüìà Most weighted terms:\")\n",
        "    for term, count in change_counts.most_common(5):\n",
        "        print(f\"   {term}: {count} times\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Run the export\n",
        "prompt_df = export_prompt_comparison()\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nüìã Preview of CSV data:\")\n",
        "print(prompt_df[['prompt_id', 'original_prompt', 'compel_enhanced_prompt']].head(3).to_string(max_colwidth=50))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Results Summary\n",
        "\n",
        "The notebook generates:\n",
        "\n",
        "### üìÅ **Files Created:**\n",
        "- `compel_results/prompt_XX_baseline.png` - Baseline generations\n",
        "- `compel_results/prompt_XX_compel.png` - Compel-enhanced generations  \n",
        "- `compel_results/comparison_XX.png` - Side-by-side comparisons\n",
        "- `compel_results/prompt_comparison.csv` - CSV with original vs enhanced prompts\n",
        "\n",
        "### üîç **What to Look For:**\n",
        "\n",
        "**Visual Differences:**\n",
        "- **Prompt adherence**: Does Compel better capture specific jewelry terms?\n",
        "- **Detail quality**: Are jewelry features more defined/accurate?\n",
        "- **Style consistency**: Modern vs vintage aesthetic differences\n",
        "\n",
        "**Quantitative Metrics:**\n",
        "- **CLIP similarity scores**: Higher = better prompt adherence\n",
        "- **Average improvement**: Overall lift from Compel weighting\n",
        "\n",
        "### üéØ **Next Steps:**\n",
        "1. **Visual inspection**: Compare side-by-side images\n",
        "2. **Quantitative analysis**: Review CLIP similarity scores\n",
        "3. **CSV analysis**: Use the exported CSV for systematic comparison\n",
        "4. **Fine-tuning**: Adjust `++` weights based on results\n",
        "5. **Integration**: If successful, integrate into main pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Pro Tip:** Try different weighting levels (`+`, `++`, `+++`) for terms that show the most improvement!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
