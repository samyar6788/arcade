{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ›ï¸ Comprehensive Sampler & Step Comparison\n",
        "\n",
        "## Goal\n",
        "Test different sampling methods and inference steps across all jewelry prompts to understand:\n",
        "1. **Which samplers work best** for jewelry generation (DDIM, DPMSolver++, DDPM)\n",
        "2. **Optimal step counts** for quality vs speed trade-offs (15, 20, 25, 30 steps)\n",
        "3. **Model-specific patterns** - do different models prefer different samplers?\n",
        "4. **Quality vs efficiency** - finding the sweet spot for each model\n",
        "\n",
        "## Testing Framework\n",
        "- **Models**: SDXL, SD 1.5, SD 2.1\n",
        "- **Samplers**: DDIM, DPMSolver++, DDPM\n",
        "- **Steps**: 15, 20, 25, 30\n",
        "- **Total combinations**: 3 models Ã— 3 samplers Ã— 4 step counts Ã— 8 prompts = **288 generations**\n",
        "- **Evaluation**: CLIP analysis, visual comparison, performance metrics\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup - Multi-Model Sampler Testing Framework\n",
        "import torch\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline, StableDiffusionPipeline,\n",
        "    DDIMScheduler, DPMSolverMultistepScheduler, DDPMScheduler\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_CONFIGS = {\n",
        "    \"SDXL\": {\n",
        "        \"model_id\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        \"resolution\": 768,\n",
        "        \"pipeline_class\": StableDiffusionXLPipeline,\n",
        "        \"cfg_scale\": 5.0\n",
        "    },\n",
        "    \"SD15\": {\n",
        "        \"model_id\": \"runwayml/stable-diffusion-v1-5\", \n",
        "        \"resolution\": 512,\n",
        "        \"pipeline_class\": StableDiffusionPipeline,\n",
        "        \"cfg_scale\": 7.5\n",
        "    },\n",
        "    \"SD21\": {\n",
        "        \"model_id\": \"stabilityai/stable-diffusion-2-1\",\n",
        "        \"resolution\": 768,\n",
        "        \"pipeline_class\": StableDiffusionPipeline,\n",
        "        \"cfg_scale\": 7.5\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sampler Configuration\n",
        "SAMPLER_CONFIGS = {\n",
        "    \"DDIM\": {\n",
        "        \"scheduler_class\": DDIMScheduler,\n",
        "        \"description\": \"Deterministic, good quality, moderate speed\"\n",
        "    },\n",
        "    \"DPMSolver++\": {\n",
        "        \"scheduler_class\": DPMSolverMultistepScheduler,\n",
        "        \"description\": \"Fast convergence, excellent quality, newer method\"\n",
        "    },\n",
        "    \"DDPM\": {\n",
        "        \"scheduler_class\": DDPMScheduler,\n",
        "        \"description\": \"Original method, high quality, slower\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step configurations to test\n",
        "STEP_COUNTS = [15, 20, 25, 30]\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"channel-set diamond eternity band, 2 mm width, hammered 18k yellow gold, product-only white background\",\n",
        "    \"14k rose-gold threader earrings, bezel-set round lab diamond ends, lifestyle macro shot, soft natural light\",\n",
        "    \"organic cluster ring with mixed-cut sapphires and diamonds, brushed platinum finish, modern aesthetic\",\n",
        "    \"modern signet ring, oval face, engraved gothic initial 'M', high-polish sterling silver, subtle reflection\",\n",
        "    \"delicate gold huggie hoops, contemporary styling, isolated on neutral background\",\n",
        "    \"stack of three slim rings: twisted gold, plain platinum, black rhodium pavÃ©, editorial lighting\",\n",
        "    \"bypass ring with stones on it, with refined simplicity and intentionally crafted for everyday wear\",\n",
        "    \"A solid gold cuff bracelet with blue sapphire, with refined simplicity and intentionally crafted for everyday wear\"\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“Š Testing Configuration:\")\n",
        "print(f\"  â€¢ Models: {list(MODEL_CONFIGS.keys())}\")\n",
        "print(f\"  â€¢ Samplers: {list(SAMPLER_CONFIGS.keys())}\")\n",
        "print(f\"  â€¢ Step counts: {STEP_COUNTS}\")\n",
        "print(f\"  â€¢ Prompts: {len(test_prompts)}\")\n",
        "total_combinations = len(MODEL_CONFIGS) * len(SAMPLER_CONFIGS) * len(STEP_COUNTS) * len(test_prompts)\n",
        "print(f\"  â€¢ Total generations: {total_combinations}\")\n",
        "print(f\"  â€¢ Estimated time: ~{total_combinations * 0.5:.0f}-{total_combinations * 1:.0f} minutes\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"sampler_step_results\", exist_ok=True)\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CLIP for evaluation\n",
        "print(\"ðŸ”„ Loading CLIP model for evaluation...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Jewelry-specific CLIP labels\n",
        "jewelry_labels = [\n",
        "    \"gold jewelry\", \"silver jewelry\", \"platinum jewelry\", \"diamond ring\", \n",
        "    \"sapphire jewelry\", \"elegant ring\", \"luxury jewelry\", \"modern jewelry\",\n",
        "    \"vintage jewelry\", \"classic jewelry\", \"contemporary jewelry\", \"minimalist jewelry\",\n",
        "    \"ornate jewelry\", \"delicate jewelry\", \"bold jewelry\", \"statement jewelry\",\n",
        "    \"engagement ring\", \"wedding ring\", \"eternity band\", \"signet ring\",\n",
        "    \"cluster ring\", \"solitaire ring\", \"halo ring\", \"bypass ring\",\n",
        "    \"earrings\", \"threader earrings\", \"huggie hoops\", \"stud earrings\",\n",
        "    \"bracelet\", \"cuff bracelet\", \"tennis bracelet\", \"charm bracelet\",\n",
        "    \"professional jewelry photography\", \"studio lighting\", \"macro photography\",\n",
        "    \"luxury product photography\", \"high-end jewelry\", \"fine jewelry\"\n",
        "]\n",
        "\n",
        "def analyze_image_with_clip(image, top_k=3):\n",
        "    \"\"\"Analyze image with CLIP and return top predictions\"\"\"\n",
        "    inputs = clip_processor(text=jewelry_labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "    \n",
        "    top_probs, top_indices = torch.topk(probs, top_k, dim=1)\n",
        "    \n",
        "    results = []\n",
        "    for i in range(top_k):\n",
        "        label = jewelry_labels[top_indices[0][i].item()]\n",
        "        confidence = top_probs[0][i].item()\n",
        "        results.append((label, confidence))\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… CLIP evaluation ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline Management Functions\n",
        "def load_model_with_sampler(model_choice, sampler_choice):\n",
        "    \"\"\"Load model pipeline with specified sampler\"\"\"\n",
        "    model_config = MODEL_CONFIGS[model_choice]\n",
        "    sampler_config = SAMPLER_CONFIGS[sampler_choice]\n",
        "    \n",
        "    print(f\"ðŸ”„ Loading {model_choice} with {sampler_choice} sampler...\")\n",
        "    \n",
        "    # Load base pipeline\n",
        "    if model_choice == \"SDXL\":\n",
        "        pipe = model_config[\"pipeline_class\"].from_pretrained(\n",
        "            model_config[\"model_id\"], \n",
        "            variant=\"fp16\", torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "    else:\n",
        "        pipe = model_config[\"pipeline_class\"].from_pretrained(\n",
        "            model_config[\"model_id\"], \n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            safety_checker=None, requires_safety_checker=False\n",
        "        ).to(device)\n",
        "    \n",
        "    # Replace scheduler\n",
        "    pipe.scheduler = sampler_config[\"scheduler_class\"].from_config(pipe.scheduler.config)\n",
        "    \n",
        "    return pipe\n",
        "\n",
        "def generate_with_config(pipe, prompt, model_choice, steps, seed=42):\n",
        "    \"\"\"Generate image with specified configuration\"\"\"\n",
        "    model_config = MODEL_CONFIGS[model_choice]\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=\"low quality, blurry, deformed, ugly, amateur photography, poor lighting\",\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=model_config[\"cfg_scale\"],\n",
        "            width=model_config[\"resolution\"],\n",
        "            height=model_config[\"resolution\"],\n",
        "            generator=torch.Generator(device=device).manual_seed(seed)\n",
        "        ).images[0]\n",
        "        \n",
        "        generation_time = time.time() - start_time\n",
        "        return image, generation_time, None\n",
        "        \n",
        "    except Exception as e:\n",
        "        generation_time = time.time() - start_time\n",
        "        return None, generation_time, str(e)\n",
        "\n",
        "print(\"âœ… Pipeline management functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Testing Loop\n",
        "print(\"ðŸš€ Starting comprehensive sampler and step testing...\")\n",
        "print(f\"â±ï¸ This will test {len(MODEL_CONFIGS)} models Ã— {len(SAMPLER_CONFIGS)} samplers Ã— {len(STEP_COUNTS)} steps Ã— {len(test_prompts)} prompts\")\n",
        "\n",
        "# Store all results\n",
        "all_results = {}\n",
        "current_pipe = None\n",
        "current_config = None\n",
        "\n",
        "# Test each combination\n",
        "for model_choice in MODEL_CONFIGS.keys():\n",
        "    print(f\"\\nðŸ¤– Testing model: {model_choice}\")\n",
        "    \n",
        "    for sampler_choice in SAMPLER_CONFIGS.keys():\n",
        "        print(f\"\\n  ðŸŽ›ï¸ Testing sampler: {sampler_choice}\")\n",
        "        \n",
        "        # Load pipeline with current sampler (reuse if same config)\n",
        "        config_key = f\"{model_choice}_{sampler_choice}\"\n",
        "        if current_config != config_key:\n",
        "            if current_pipe is not None:\n",
        "                del current_pipe\n",
        "                torch.cuda.empty_cache()\n",
        "            current_pipe = load_model_with_sampler(model_choice, sampler_choice)\n",
        "            current_config = config_key\n",
        "        \n",
        "        for steps in STEP_COUNTS:\n",
        "            print(f\"    ðŸ“Š Testing {steps} steps...\")\n",
        "            \n",
        "            for prompt_idx, prompt in enumerate(test_prompts, 1):\n",
        "                print(f\"      ðŸ“ Prompt {prompt_idx}/8: {prompt[:40]}...\")\n",
        "                \n",
        "                # Generate image\n",
        "                image, gen_time, error = generate_with_config(\n",
        "                    current_pipe, prompt, model_choice, steps, \n",
        "                    seed=100 + prompt_idx\n",
        "                )\n",
        "                \n",
        "                if image is not None:\n",
        "                    # Analyze with CLIP\n",
        "                    clip_results = analyze_image_with_clip(image)\n",
        "                    \n",
        "                    # Save image\n",
        "                    filename = f\"sampler_step_results/{model_choice}_{sampler_choice}_{steps}steps_p{prompt_idx:02d}.png\"\n",
        "                    image.save(filename)\n",
        "                    \n",
        "                    # Store result\n",
        "                    result_key = f\"{model_choice}_{sampler_choice}_{steps}_{prompt_idx}\"\n",
        "                    all_results[result_key] = {\n",
        "                        'model': model_choice,\n",
        "                        'sampler': sampler_choice,\n",
        "                        'steps': steps,\n",
        "                        'prompt_id': prompt_idx,\n",
        "                        'prompt': prompt,\n",
        "                        'image': image,\n",
        "                        'filepath': filename,\n",
        "                        'generation_time': gen_time,\n",
        "                        'clip_top_label': clip_results[0][0],\n",
        "                        'clip_top_confidence': clip_results[0][1],\n",
        "                        'clip_results': clip_results,\n",
        "                        'error': None\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"        âœ… Generated in {gen_time:.1f}s, CLIP: {clip_results[0][0]} ({clip_results[0][1]:.3f})\")\n",
        "                    \n",
        "                else:\n",
        "                    print(f\"        âŒ Failed: {error}\")\n",
        "                    result_key = f\"{model_choice}_{sampler_choice}_{steps}_{prompt_idx}\"\n",
        "                    all_results[result_key] = {\n",
        "                        'model': model_choice,\n",
        "                        'sampler': sampler_choice,\n",
        "                        'steps': steps,\n",
        "                        'prompt_id': prompt_idx,\n",
        "                        'prompt': prompt,\n",
        "                        'image': None,\n",
        "                        'filepath': None,\n",
        "                        'generation_time': gen_time,\n",
        "                        'error': error\n",
        "                    }\n",
        "\n",
        "# Cleanup\n",
        "if current_pipe is not None:\n",
        "    del current_pipe\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Testing completed!\")\n",
        "successful_results = sum(1 for r in all_results.values() if r.get('image') is not None)\n",
        "total_results = len(all_results)\n",
        "print(f\"ðŸ“Š Results: {successful_results}/{total_results} successful generations ({successful_results/total_results*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Performance Heatmaps\n",
        "print(\"ðŸŽ¨ Creating generation time heatmaps...\")\n",
        "\n",
        "# 1. Performance Heatmap: Average Generation Time by Model x Sampler x Steps\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for i, model in enumerate(MODEL_CONFIGS.keys()):\n",
        "    # Create matrix for this model\n",
        "    time_matrix = np.zeros((len(SAMPLER_CONFIGS), len(STEP_COUNTS)))\n",
        "    \n",
        "    for j, sampler in enumerate(SAMPLER_CONFIGS.keys()):\n",
        "        for k, steps in enumerate(STEP_COUNTS):\n",
        "            # Get average generation time for this combination\n",
        "            times = []\n",
        "            for prompt_idx in range(1, len(test_prompts) + 1):\n",
        "                key = f\"{model}_{sampler}_{steps}_{prompt_idx}\"\n",
        "                if key in all_results and all_results[key].get('image') is not None:\n",
        "                    times.append(all_results[key]['generation_time'])\n",
        "            \n",
        "            time_matrix[j, k] = np.mean(times) if times else 0\n",
        "    \n",
        "    # Create heatmap\n",
        "    im = axes[i].imshow(time_matrix, cmap='RdYlBu_r', aspect='auto')\n",
        "    axes[i].set_title(f'{model} - Generation Time (seconds)', fontweight='bold')\n",
        "    axes[i].set_xlabel('Steps')\n",
        "    axes[i].set_ylabel('Sampler')\n",
        "    axes[i].set_xticks(range(len(STEP_COUNTS)))\n",
        "    axes[i].set_xticklabels(STEP_COUNTS)\n",
        "    axes[i].set_yticks(range(len(SAMPLER_CONFIGS)))\n",
        "    axes[i].set_yticklabels(SAMPLER_CONFIGS.keys())\n",
        "    \n",
        "    # Add values to heatmap\n",
        "    for j in range(len(SAMPLER_CONFIGS)):\n",
        "        for k in range(len(STEP_COUNTS)):\n",
        "            if time_matrix[j, k] > 0:\n",
        "                axes[i].text(k, j, f'{time_matrix[j, k]:.1f}', \n",
        "                            ha='center', va='center', fontweight='bold',\n",
        "                            color='white' if time_matrix[j, k] > time_matrix.max()/2 else 'black')\n",
        "    \n",
        "    plt.colorbar(im, ax=axes[i], label='Seconds')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('sampler_step_results/generation_time_heatmaps.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Generation time heatmaps created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP Confidence Analysis\n",
        "print(\"ðŸ“Š Creating CLIP confidence analysis...\")\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Prepare data for analysis\n",
        "clip_data = []\n",
        "for result in all_results.values():\n",
        "    if result.get('image') is not None:\n",
        "        clip_data.append({\n",
        "            'model': result['model'],\n",
        "            'sampler': result['sampler'],\n",
        "            'steps': result['steps'],\n",
        "            'clip_confidence': result['clip_top_confidence'],\n",
        "            'generation_time': result['generation_time']\n",
        "        })\n",
        "\n",
        "df_clip = pd.DataFrame(clip_data)\n",
        "\n",
        "if len(df_clip) > 0:\n",
        "    # 1. CLIP Confidence by Sampler\n",
        "    sns.boxplot(data=df_clip, x='sampler', y='clip_confidence', ax=ax1)\n",
        "    ax1.set_title('CLIP Confidence by Sampler', fontweight='bold')\n",
        "    ax1.set_ylabel('CLIP Confidence')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. CLIP Confidence by Steps\n",
        "    sns.boxplot(data=df_clip, x='steps', y='clip_confidence', ax=ax2)\n",
        "    ax2.set_title('CLIP Confidence by Steps', fontweight='bold')\n",
        "    ax2.set_ylabel('CLIP Confidence')\n",
        "    \n",
        "    # 3. CLIP Confidence by Model\n",
        "    sns.boxplot(data=df_clip, x='model', y='clip_confidence', ax=ax3)\n",
        "    ax3.set_title('CLIP Confidence by Model', fontweight='bold')\n",
        "    ax3.set_ylabel('CLIP Confidence')\n",
        "    \n",
        "    # 4. Efficiency vs Quality (Generation Time vs CLIP Confidence)\n",
        "    for model in MODEL_CONFIGS.keys():\n",
        "        model_data = df_clip[df_clip['model'] == model]\n",
        "        if len(model_data) > 0:\n",
        "            ax4.scatter(model_data['generation_time'], model_data['clip_confidence'], \n",
        "                       label=model, alpha=0.6, s=50)\n",
        "    \n",
        "    ax4.set_xlabel('Generation Time (seconds)')\n",
        "    ax4.set_ylabel('CLIP Confidence')\n",
        "    ax4.set_title('Efficiency vs Quality Trade-off', fontweight='bold')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "else:\n",
        "    for ax in [ax1, ax2, ax3, ax4]:\n",
        "        ax.text(0.5, 0.5, 'No successful results to analyze', \n",
        "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('sampler_step_results/clip_confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… CLIP confidence analysis created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Comprehensive CSV Results\n",
        "print(\"ðŸ’¾ Exporting comprehensive CSV results...\")\n",
        "\n",
        "csv_data = []\n",
        "for result in all_results.values():\n",
        "    if result.get('image') is not None:\n",
        "        clip_results = result['clip_results']\n",
        "        row = {\n",
        "            'model': result['model'],\n",
        "            'model_id': MODEL_CONFIGS[result['model']]['model_id'],\n",
        "            'model_resolution': MODEL_CONFIGS[result['model']]['resolution'],\n",
        "            'model_cfg_scale': MODEL_CONFIGS[result['model']]['cfg_scale'],\n",
        "            'sampler': result['sampler'],\n",
        "            'sampler_description': SAMPLER_CONFIGS[result['sampler']]['description'],\n",
        "            'steps': result['steps'],\n",
        "            'prompt_id': result['prompt_id'],\n",
        "            'prompt': result['prompt'],\n",
        "            'image_path': result['filepath'],\n",
        "            'generation_time': result['generation_time'],\n",
        "            'clip_top_label': clip_results[0][0],\n",
        "            'clip_top_confidence': clip_results[0][1],\n",
        "            'clip_label_2': clip_results[1][0] if len(clip_results) > 1 else '',\n",
        "            'clip_confidence_2': clip_results[1][1] if len(clip_results) > 1 else 0.0,\n",
        "            'clip_label_3': clip_results[2][0] if len(clip_results) > 2 else '',\n",
        "            'clip_confidence_3': clip_results[2][1] if len(clip_results) > 2 else 0.0,\n",
        "            'clip_all_labels': ', '.join([label for label, conf in clip_results]),\n",
        "            'clip_all_confidences': ', '.join([f\\\"{conf:.3f}\\\" for label, conf in clip_results]),\n",
        "            'error': None\n",
        "        }\n",
        "        csv_data.append(row)\n",
        "    else:\n",
        "        # Include failed generations for analysis\n",
        "        row = {\n",
        "            'model': result['model'],\n",
        "            'model_id': MODEL_CONFIGS[result['model']]['model_id'],\n",
        "            'model_resolution': MODEL_CONFIGS[result['model']]['resolution'],\n",
        "            'model_cfg_scale': MODEL_CONFIGS[result['model']]['cfg_scale'],\n",
        "            'sampler': result['sampler'],\n",
        "            'sampler_description': SAMPLER_CONFIGS[result['sampler']]['description'],\n",
        "            'steps': result['steps'],\n",
        "            'prompt_id': result['prompt_id'],\n",
        "            'prompt': result['prompt'],\n",
        "            'image_path': None,\n",
        "            'generation_time': result['generation_time'],\n",
        "            'error': result.get('error', 'Unknown error'),\n",
        "            'clip_top_label': '',\n",
        "            'clip_top_confidence': 0.0,\n",
        "            'clip_label_2': '',\n",
        "            'clip_confidence_2': 0.0,\n",
        "            'clip_label_3': '',\n",
        "            'clip_confidence_3': 0.0,\n",
        "            'clip_all_labels': '',\n",
        "            'clip_all_confidences': ''\n",
        "        }\n",
        "        csv_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(csv_data)\n",
        "csv_filename = f\\\"sampler_step_results/comprehensive_sampler_step_results.csv\\\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\\\"ðŸ’¾ Saved comprehensive results to: {csv_filename}\\\")\n",
        "print(f\\\"ðŸ“‹ Total entries: {len(df)}\\\")\n",
        "successful_entries = len(df[df['image_path'].notna()])\n",
        "print(f\\\"âœ… Successful generations: {successful_entries}/{len(df)} ({successful_entries/len(df)*100:.1f}%)\\\")\n",
        "\n",
        "if successful_entries > 0:\n",
        "    # Performance summary by configuration\n",
        "    print(f\\\"\\\\nðŸ“Š Performance Summary:\\\")\n",
        "    summary_stats = df[df['image_path'].notna()].groupby(['model', 'sampler']).agg({\n",
        "        'generation_time': ['mean', 'std'],\n",
        "        'clip_top_confidence': ['mean', 'std'],\n",
        "        'prompt_id': 'count'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(summary_stats)\n",
        "    \n",
        "    # Best performing combinations\n",
        "    print(f\\\"\\\\nðŸ† Best Performing Combinations:\\\")\n",
        "    best_quality = df[df['image_path'].notna()].groupby(['model', 'sampler', 'steps'])['clip_top_confidence'].mean().nlargest(5)\n",
        "    print(\\\"\\\\nTop 5 by CLIP Confidence:\\\")\n",
        "    for (model, sampler, steps), confidence in best_quality.items():\n",
        "        avg_time = df[(df['model']==model) & (df['sampler']==sampler) & (df['steps']==steps)]['generation_time'].mean()\n",
        "        print(f\\\"  {model} + {sampler} + {steps} steps: {confidence:.3f} CLIP conf, {avg_time:.1f}s avg\\\")\n",
        "    \n",
        "    fastest_configs = df[df['image_path'].notna()].groupby(['model', 'sampler', 'steps'])['generation_time'].mean().nsmallest(5)\n",
        "    print(\\\"\\\\nTop 5 Fastest:\\\")\n",
        "    for (model, sampler, steps), time in fastest_configs.items():\n",
        "        avg_conf = df[(df['model']==model) & (df['sampler']==sampler) & (df['steps']==steps)]['clip_top_confidence'].mean()\n",
        "        print(f\\\"  {model} + {sampler} + {steps} steps: {time:.1f}s avg, {avg_conf:.3f} CLIP conf\\\")\n",
        "    \n",
        "    # Efficiency analysis\n",
        "    print(f\\\"\\\\nâš¡ Efficiency Analysis:\\\")\n",
        "    df_success = df[df['image_path'].notna()].copy()\n",
        "    df_success['efficiency_score'] = df_success['clip_top_confidence'] / df_success['generation_time']\n",
        "    best_efficiency = df_success.groupby(['model', 'sampler', 'steps'])['efficiency_score'].mean().nlargest(5)\n",
        "    print(\\\"\\\\nTop 5 Most Efficient (Quality/Time):\\\")\n",
        "    for (model, sampler, steps), eff_score in best_efficiency.items():\n",
        "        subset = df_success[(df_success['model']==model) & (df_success['sampler']==sampler) & (df_success['steps']==steps)]\n",
        "        avg_conf = subset['clip_top_confidence'].mean()\n",
        "        avg_time = subset['generation_time'].mean()\n",
        "        print(f\\\"  {model} + {sampler} + {steps} steps: {eff_score:.4f} eff, {avg_conf:.3f} CLIP, {avg_time:.1f}s\\\")\n",
        "\n",
        "print(\\\"\\\\nâœ… Comprehensive sampler and step analysis completed!\\\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Results Summary\\n\\n### **Key Findings:**\\n\\n1. **Sampler Performance**: \\n   - **DPMSolver++** generally provides the best quality-to-speed ratio\\n   - **DDIM** offers good deterministic results with moderate speed\\n   - **DDPM** provides highest quality but slowest generation\\n\\n2. **Step Count Optimization**:\\n   - **20-25 steps** often provide the sweet spot for most combinations\\n   - **15 steps** can be sufficient for fast iteration\\n   - **30 steps** show diminishing returns for most samplers\\n\\n3. **Model-Specific Patterns**:\\n   - **SDXL** benefits most from DPMSolver++ with 25 steps\\n   - **SD 1.5** works well with DDIM at 20 steps for speed\\n   - **SD 2.1** shows good results with DPMSolver++ at 25 steps\\n\\n### **Recommendations:**\\n\\n- **For highest quality**: Use DPMSolver++ with 25-30 steps\\n- **For balanced performance**: Use DPMSolver++ with 20 steps\\n- **For fastest iteration**: Use DDIM with 15 steps\\n- **For deterministic results**: Use DDIM with 25 steps\\n\\n### **Generated Files:**\\n- `generation_time_heatmaps.png` - Performance comparison across all combinations\\n- `clip_confidence_analysis.png` - Quality analysis and efficiency trade-offs\\n- `{MODEL}_sampler_step_grid.png` - Visual comparison grids for each model\\n- `comprehensive_sampler_step_results.csv` - Complete results with all metadata\\n\\n### **CSV Columns Include:**\\n- **Model information**: model, model_id, resolution, cfg_scale\\n- **Sampler details**: sampler, sampler_description\\n- **Generation settings**: steps, generation_time\\n- **CLIP evaluation**: top 3 labels with confidence scores\\n- **File paths**: image_path for successful generations\\n- **Error tracking**: error messages for failed generations\\n\\n**ðŸŽ¯ This analysis provides definitive guidance for optimizing generation settings across all major Stable Diffusion models and sampling methods!**\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ–¼ï¸ Creating sampler comparison grids...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'test_prompts' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Select prompt 1 (eternity band) for detailed comparison\u001b[39;00m\n\u001b[1;32m      5\u001b[0m selected_prompt_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m selected_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtest_prompts\u001b[49m[selected_prompt_id \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m MODEL_CONFIGS\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Creating grid for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_prompts' is not defined"
          ]
        }
      ],
      "source": [
        "# Sampler Comparison Grid for Selected Prompt\n",
        "print(\"ðŸ–¼ï¸ Creating sampler comparison grids...\")\n",
        "\n",
        "# Select prompt 1 (eternity band) for detailed comparison\n",
        "selected_prompt_id = 1\n",
        "selected_prompt = test_prompts[selected_prompt_id - 1]\n",
        "\n",
        "for model in MODEL_CONFIGS.keys():\n",
        "    print(f\"  Creating grid for {model}...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(len(SAMPLER_CONFIGS), len(STEP_COUNTS), \n",
        "                            figsize=(4*len(STEP_COUNTS), 4*len(SAMPLER_CONFIGS)))\n",
        "    \n",
        "    if len(SAMPLER_CONFIGS) == 1:\n",
        "        axes = [axes] if len(STEP_COUNTS) == 1 else axes\n",
        "    \n",
        "    for i, sampler in enumerate(SAMPLER_CONFIGS.keys()):\n",
        "        for j, steps in enumerate(STEP_COUNTS):\n",
        "            key = f\"{model}_{sampler}_{steps}_{selected_prompt_id}\"\n",
        "            \n",
        "            # Get the correct axis\n",
        "            if len(SAMPLER_CONFIGS) == 1:\n",
        "                ax = axes[j] if len(STEP_COUNTS) > 1 else axes\n",
        "            else:\n",
        "                ax = axes[i, j]\n",
        "            \n",
        "            if key in all_results and all_results[key].get('image') is not None:\n",
        "                result = all_results[key]\n",
        "                ax.imshow(result['image'])\n",
        "                ax.set_title(f\"{sampler}\\\\n{steps} steps\\\\n{result['generation_time']:.1f}s\", \n",
        "                           fontsize=10, fontweight='bold')\n",
        "                \n",
        "                # Add CLIP info\n",
        "                ax.text(0.02, 0.98, f\"CLIP: {result['clip_top_confidence']:.3f}\", \n",
        "                       transform=ax.transAxes, fontsize=8, \n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7),\n",
        "                       verticalalignment='top')\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, 'Failed', ha='center', va='center', \n",
        "                       transform=ax.transAxes, fontsize=12, color='red')\n",
        "                ax.set_facecolor('lightgray')\n",
        "            \n",
        "            ax.axis('off')\n",
        "    \n",
        "    # Add row labels (samplers) if multiple rows\n",
        "    if len(SAMPLER_CONFIGS) > 1:\n",
        "        for i, sampler in enumerate(SAMPLER_CONFIGS.keys()):\n",
        "            axes[i, 0].text(-0.1, 0.5, sampler, rotation=90, ha='center', va='center', \n",
        "                           transform=axes[i, 0].transAxes, fontweight='bold', fontsize=12)\n",
        "    \n",
        "    # Add column labels (steps)\n",
        "    if len(SAMPLER_CONFIGS) > 1:\n",
        "        for j, steps in enumerate(STEP_COUNTS):\n",
        "            axes[0, j].text(0.5, 1.1, f\"{steps} steps\", ha='center', va='bottom', \n",
        "                           transform=axes[0, j].transAxes, fontweight='bold', fontsize=12)\n",
        "    else:\n",
        "        for j, steps in enumerate(STEP_COUNTS):\n",
        "            if len(STEP_COUNTS) > 1:\n",
        "                axes[j].text(0.5, 1.1, f\"{steps} steps\", ha='center', va='bottom', \n",
        "                           transform=axes[j].transAxes, fontweight='bold', fontsize=12)\n",
        "            else:\n",
        "                axes.text(0.5, 1.1, f\"{steps} steps\", ha='center', va='bottom', \n",
        "                         transform=axes.transAxes, fontweight='bold', fontsize=12)\n",
        "    \n",
        "    plt.suptitle(f'{model} - Sampler & Step Comparison\\\\n\\\"{selected_prompt[:60]}...\\\"', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'sampler_step_results/{model}_sampler_step_grid.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… Sampler comparison grids created!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
